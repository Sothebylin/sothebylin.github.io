
[{"content":"","date":"27 March 2025","externalUrl":null,"permalink":"/categories/deeplearning/","section":"文章分类","summary":"","title":"DeepLearning","type":"categories"},{"content":"","date":"27 March 2025","externalUrl":null,"permalink":"/series/fastai-course-learning-/","section":"Series","summary":"","title":"Fastai Course Learning ","type":"series"},{"content":" Learning neural network further # 本文复现的是fastai的官方视频教程 点击此处跳转\n其官方kaggle笔记本 点击此处跳转\n本篇博客是在fastai课程基础上进行总结，基于kaggle上的paddy disease classificatin比赛，使用了Resnet框架和Convnext框架实现对疾病的分类，结果发现Convnext比Resnext效果更好，并且使用了测试时间增强（TTA）、扩大训练规模、对图像进行填充等方式探究降低误差的效果，最后还构建了多输出模型，输出疾病以及种类。\nSetup # !pip install -Uq fastkaggle from fastai import * from fastkaggle import * comp = \u0026#39;paddy-disease-classification\u0026#39; path = setup_comp(comp, install=\u0026#39;fastai \u0026#34;timm\u0026gt;=0.6.2.dev0\u0026#34;\u0026#39;) path.ls() (#4) [Path(\u0026rsquo;../input/paddy-disease-classification/sample_submission.csv\u0026rsquo;),Path(\u0026rsquo;../input/paddy-disease-classification/train_images\u0026rsquo;),Path(\u0026rsquo;../input/paddy-disease-classification/train.csv\u0026rsquo;),Path(\u0026rsquo;../input/paddy-disease-classification/test_images\u0026rsquo;)]\nfrom fastai.vision.all import * trn_path = path/\u0026#39;train_images\u0026#39; imgfiles = get_image_files(trn_path) img = PILImage.create(imgfiles[0]) print(img.size) img.to_thumb(128) (480, 640)\n不过要注意的是PIL展示的图像尺寸格式\n库/方法 尺寸格式 备注 PIL / PILImage (宽度, 高度) 直接对应图像的实际像素尺寸 OpenCV (高度, 宽度) OpenCV 使用 (h, w) 的 NumPy 风格 matplotlib (宽度, 高度) 与 PIL 一致 PyTorch (通道, 高度, 宽度) 张量格式（需额外注意） 为检查所有图像的尺寸采用fastcore中的parallel模块并行处理\nfrom fastcore.parallel import * def f(o): return PILImage.create(o).size sizes = parallel(f, imgfiles, n_workers=8) pd.Series(sizes).value_counts() (480, 640) 10403 (640, 480) 4 Name: count, dtype: int64\ndls = ImageDataLoaders.from_folder( trn_path, valid_pct=0.2, #当未划分训练集和验证集时需要传递valid_pct参数 seed=42, item_tfms=Resize(480,method=\u0026#39;squish\u0026#39;), batch_tfms=aug_transforms(size=128,min_scale=0.75) ) item_tfms 是 ImageDataLoaders.from_folder() 方法的一个参数名，全称为 ​item transformations​（单样本变换）。它用于定义在数据加载时对单个图像（item）应用的预处理或增强操作。\ndls.show_batch(max_n=6) fastai 的 from_folder 方式能从子文件夹名自动提取类别。\n采用Resnet26d训练 # 定义学习模型并查找学习率 # learn = vision_learner( dls, \u0026#39;resnet26d\u0026#39;, metrics=error_rate, path=\u0026#39;.\u0026#39;).to_fp16() model.safetensors: 0%| | 0.00/64.2M [00:00\u0026lt;?, ?B/s]\n参数 作用 示例值/类型 补充说明 dls 数据加载器，包含训练集和验证集 ImageDataLoaders对象 需通过 DataBlock 或工厂函数（如 ImageDataLoaders.from_folder）生成 'resnet26d' 模型架构（ResNet的变体，平衡速度和精度） 字符串或nn.Module 其他选项：'resnet34', 'efficientnet_b0', 'convnext_tiny' 等 metrics 训练时计算的评估指标（支持单指标或列表） error_rate 常用指标：\n• accuracy\n• Precision\n• Recall\n• F1Score\n• RocAuc（二分类）\n• Perplexity（语言模型） path 模型和日志的保存目录（.表示当前目录） 字符串或Path对象 保存内容包括：\n• 模型参数（.pth）\n• 训练日志（history.csv） .to_fp16() 启用混合精度训练（FP16），加速训练并减少显存占用 方法调用 需GPU支持（如NVIDIA Volta+架构）\n可用 .to_fp32() 强制禁用 查找一下最佳的学习率\nlearn.lr_find(suggest_funcs=(valley,slide)) SuggestedLRs(valley=0.0010000000474974513, slide=0.0020892962347716093)\nsuggest_funcs 参数：指定学习率推荐策略\nvalley：选择损失下降最陡峭的点（避免过大的学习率）。\nslide：选择损失开始平稳上升的点（保守但稳定）。\nResnet26d模型训练 # learn.fine_tune(3,0.01) epoch train_loss valid_loss error_rate time 0 1.783480 1.293824 0.423835 00:42 epoch train_loss valid_loss error_rate time 0 1.128158 0.715136 0.238827 00:42 1 0.802069 0.490330 0.160980 00:42 2 0.569914 0.385769 0.135992 00:42 fine_tune() 是 fastai 提供的微调方法，分两阶段自动训练：\n1)冻结阶段（Freeze）​\n仅训练新增的头部层​（替换后的最后一层），预训练主干层保持冻结。 默认1轮​（可通过 freeze_epochs 修改）。 学习率自动设为 base_lr/10（若未指定 base_lr，则用传入的 0.01）。\n2)解冻阶段（Unfreeze）​\n解冻所有层，全部参与训练。 训练剩余轮数​（此处 3 - 1 = 2 轮）。 使用传入的学习率 0.01（或通过 lr_find() 优化的值。\nfastai 的 vision_learner 会自动从数据加载器 (dls) 中推断输出维度\n新增的头部层（New Head Layers）​ 是指当使用预训练模型（如ResNet、EfficientNet等）进行迁移学习时，​替换或添加的最后一层（或几层）网络结构，用于适配当前任务的输出需求\n将训练好的Resnet26d应用到测试集上 # tst_files = get_image_files(path/\u0026#39;test_images\u0026#39;).sorted() tst_dl = dls.test_dl(tst_files) test_dl() 是 ​fastai 库中 DataLoaders 对象的一个方法，专门用于为测试集（或推理数据）​创建一个与训练时数据预处理一致的数据加载器\nprobs,_,idxs = learn.get_preds(dl=tst_dl, with_decoded=True) idxs tensor([7, 8, 4, \u0026hellip;, 8, 1, 5])\nmapping = dict(enumerate(dls.vocab)) results = pd.Series(idxs.numpy(), name=\u0026#34;idxs\u0026#34;).map(mapping) results 0 hispa 1 normal 2 brown_spot 3 blast 4 blast \u0026hellip; 3464 dead_heart 3465 hispa 3466 normal 3467 bacterial_leaf_streak 3468 dead_heart Name: idxs, Length: 3469, dtype: object\nss = pd.read_csv(path/\u0026#39;sample_submission.csv\u0026#39;) ss[\u0026#39;label\u0026#39;] = results ss.to_csv(\u0026#39;subm.csv\u0026#39;, index=False) !head subm.csv image_id,label 200001.jpg,hispa 200002.jpg,normal 200003.jpg,brown_spot 200004.jpg,blast 200005.jpg,blast 200006.jpg,brown_spot 200007.jpg,dead_heart 200008.jpg,brown_spot 200009.jpg,hispa\n改进一下Resnet26d # 为了让模型训练的更快，我们可以将图像的大小缩小（让像素数量减少4倍）\ntrn_path = Path(\u0026#39;sml\u0026#39;) resize_images(path/\u0026#39;train_images\u0026#39;, dest=trn_path, max_size=256, recurse=True) fastai 有一个函数可以做到这一点，同时保持数据的文件夹结构即resize_images\ndls = ImageDataLoaders.from_folder(trn_path, valid_pct=0.2, seed=42, item_tfms=Resize((256,192))) dls.show_batch(max_n=3) 将之前训练的步骤封装起来\ndef train(arch, item, batch, epochs=5): dls = ImageDataLoaders.from_folder(trn_path, seed=42, valid_pct=0.2, item_tfms=item, batch_tfms=batch) learn = vision_learner(dls, arch, metrics=error_rate).to_fp16() learn.fine_tune(epochs, 0.01) return learn learn = train(\u0026#39;resnet26d\u0026#39;, item=Resize(192), batch=aug_transforms(size=128, min_scale=0.75)) epoch train_loss valid_loss error_rate time 0 1.942248 1.426470 0.463239 00:17 epoch train_loss valid_loss error_rate time 0 1.289961 0.999535 0.333974 00:19 1 1.018504 0.688591 0.216242 00:18 2 0.740462 0.482486 0.160019 00:18 3 0.529927 0.406155 0.124459 00:18 4 0.435775 0.372298 0.116771 00:18 可以看到准确率是有了提升\n使用ConNeXt_small模型 # 定义convnext_small_in22k学习模型 # arch = \u0026#39;convnext_small_in22k\u0026#39; learn = train(arch, item=Resize(192, method=\u0026#39;squish\u0026#39;), batch=aug_transforms(size=128, min_scale=0.75)) epoch train_loss valid_loss error_rate time 0 1.245993 0.795927 0.255166 00:55 epoch train_loss valid_loss error_rate time 0 0.647190 0.434121 0.148967 00:53 1 0.477503 0.379062 0.119654 00:53 2 0.300822 0.191345 0.060548 00:52 3 0.192040 0.134807 0.039404 00:52 4 0.127018 0.125794 0.036521 00:52 损失降低了很多，Convnext模型在这个数据集上的效果要比Resnet好\n对图像进行填充处理 # dls = ImageDataLoaders.from_folder(trn_path, valid_pct=0.2, seed=42, item_tfms=Resize(192, method=ResizeMethod.Pad, pad_mode=PadMode.Zeros)) dls.show_batch(max_n=3) 再次进行训练 # learn = train(arch, item=Resize((256,192), method=ResizeMethod.Pad, pad_mode=PadMode.Zeros), batch=aug_transforms(size=(171,128), min_scale=0.75)) epoch train_loss valid_loss error_rate time 0 1.225350 0.838977 0.261413 00:44 epoch train_loss valid_loss error_rate time 0 0.647279 0.475882 0.153292 00:51 1 0.532679 0.375510 0.116290 00:51 2 0.348923 0.203416 0.067275 00:50 3 0.207750 0.132006 0.037001 00:50 4 0.140231 0.124632 0.035079 00:50 好像效果也没很大的提升:(\n应用测试时间增强Test time augmentation（TTA） # During inference or validation, creating multiple versions of each image, using data augmentation, and then taking the average or maximum of the predictions for each augmented version of the image. 在推理或验证期间，使用数据增强创建每个图像的多个版本，然后对图像的每个增强版本取预测的平均值或最大值。\n由上面的训练可知在没有TTA的情况下错误率是：\nvalid = learn.dls.valid preds,targs = learn.get_preds(dl=valid) error_rate(preds, targs) TensorBase(0.0351)\ntta_preds,_ = learn.tta(dl=valid) error_rate(tta_preds, targs) TensorBase(0.0332)\n错误率有降低！\n将ConvNext应用的规模扩大 # trn_path = path/\u0026#39;train_images\u0026#39; learn = train(arch, epochs=12, item=Resize((480, 360), method=ResizeMethod.Pad, pad_mode=PadMode.Zeros), batch=aug_transforms(size=(256,192), min_scale=0.75)) epoch train_loss valid_loss error_rate time 0 1.097865 0.666030 0.218645 01:10 epoch train_loss valid_loss error_rate time 0 0.530017 0.320848 0.103316 01:27 1 0.399917 0.254228 0.077367 01:28 2 0.354157 0.281102 0.087938 01:27 3 0.306803 0.235106 0.074003 01:27 4 0.222612 0.154293 0.043248 01:28 5 0.158987 0.174899 0.046612 01:27 6 0.127389 0.122164 0.035079 01:28 7 0.089526 0.120237 0.030754 01:27 8 0.070281 0.117129 0.032196 01:28 9 0.052415 0.094729 0.024507 01:27 10 0.044216 0.091565 0.024027 01:28 11 0.034039 0.091716 0.023546 01:28 应用tta\ntta_preds,targs = learn.tta(dl=learn.dls.valid) error_rate(tta_preds, targs) TensorBase(0.0226)\n可以看到是降低了损失率的\n将ConvNext应用到测试集上 # tst_files = get_image_files(path/\u0026#39;test_images\u0026#39;).sorted() tst_dl = learn.dls.test_dl(tst_files) preds,_ = learn.tta(dl=tst_dl) idxs = preds.argmax(dim=1) 使用PyTorch 中的 argmax来选取出最大概率所在的标签，dim=1即沿类别维度（每行）计算，返回每个样本预测概率最高的类别索引\nvocab = np.array(learn.dls.vocab) # 使用vocab映射 results = pd.Series(vocab[idxs], name=\u0026#34;idxs\u0026#34;) 在 fastai 中，learn.dls.vocab 是数据加载器 (DataLoaders) 的类别标签列表，用于将模型输出的预测索引（数字）映射回可读的类别名称（如字符串）\nss = pd.read_csv(path/\u0026#39;sample_submission.csv\u0026#39;) ss[\u0026#39;label\u0026#39;] = results ss.to_csv(\u0026#39;subm2.csv\u0026#39;, index=False) !head subm2.csv image_id,label 200001.jpg,hispa 200002.jpg,normal 200003.jpg,blast 200004.jpg,blast 200005.jpg,blast 200006.jpg,brown_spot 200007.jpg,dead_heart 200008.jpg,brown_spot 200009.jpg,hispa\n创建多输出的模型 # 创建模型准备 # df = pd.read_csv(path/\u0026#39;train.csv\u0026#39;, index_col=\u0026#39;image_id\u0026#39;) df.head() label variety age image_id 100330.jpg bacterial_leaf_blight ADT45 45 100365.jpg bacterial_leaf_blight ADT45 45 100382.jpg bacterial_leaf_blight ADT45 45 100632.jpg bacterial_leaf_blight ADT45 45 101918.jpg bacterial_leaf_blight ADT45 45 我们让image_id作为索引，方便查找\nDataBlock使用 get_image_files 来获取训练图像列表，该列表返回 Path 对象，而如果要查找某个项目以获取其种类，我们需要传递其 name 。这是一个执行此操作的函数：\ndef get_variety(p): return df.loc[p.name, \u0026#39;variety\u0026#39;] dls = DataBlock( blocks=(ImageBlock,CategoryBlock,CategoryBlock), n_inp=1, get_items=get_image_files, get_y = [parent_label,get_variety], splitter=RandomSplitter(0.2, seed=42), item_tfms=Resize(192, method=\u0026#39;squish\u0026#39;), batch_tfms=aug_transforms(size=128, min_scale=0.75) ).dataloaders(trn_path) 使用 DataBlock API，这是一种灵活且方便的方式，可以将数据处理管道的各个部分连接在一起 DataBlock 将从每个文件中创建 3 个内容：一个图像（文件的内容）和 2 个分类变量（疾病和品种）。\ndls.show_batch(max_n=6) 定义多输出的学习模型 # 多输出的关键的区别在于，我们的指标和损失现在将接收三个东西而不是两个：模型输出（即指标和损失函数输入）和两个目标（疾病和多样性）。因此，我们需要重新定义指标（ error_rate ）和损失函数（ cross_entropy ），以传递 disease 目标：\narch = \u0026#39;convnext_small_in22k\u0026#39; lr = 0.01 为了预测每种疾病和每种疾病的概率，我们现在需要模型输出一个长度为 20 的张量，因为有 10 种可能的疾病和 10 种可能的疾病。我们可以通过设置 n_out=20\nlearn = vision_learner(dls, arch, n_out=20).to_fp16() 因为要预测两个输出了，之前的损失函数是只针对疾病的，在这里损失函数使用的是cross_entropy即交叉熵损失函数\ndef disease_loss(inp,disease,variety): return F.cross_entropy(inp[:,:10],disease) def variety_loss(inp,disease,variety): return F.cross_entropy(inp[:,10:],variety) def combine_loss(inp,disease,variety):return disease_loss(inp,disease,variety)+variety_loss(inp,disease,variety) def disease_err(inp,disease,variety): return error_rate(inp[:,:10],disease) def variety_err(inp,disease,variety): return error_rate(inp[:,10:],variety) err_metrics = (disease_err,variety_err) all_metrics = err_metrics+(disease_loss,variety_loss) 多输出模型训练 # learn = vision_learner(dls, arch, loss_func=combine_loss, metrics=all_metrics, n_out=20).to_fp16() learn.fine_tune(5, lr) epoch train_loss valid_loss disease_err variety_err disease_loss variety_loss time 0 2.276296 1.189668 0.257088 0.128304 0.783521 0.406147 00:51 epoch train_loss valid_loss disease_err variety_err disease_loss variety_loss time 0 1.005188 0.580068 0.127823 0.059106 0.387363 0.192705 00:54 1 0.757706 0.387292 0.094666 0.036040 0.280007 0.107285 00:53 2 0.477868 0.262185 0.061509 0.022585 0.193384 0.068801 00:54 3 0.269435 0.171966 0.040846 0.009611 0.134649 0.037317 00:54 4 0.200365 0.163008 0.037963 0.009130 0.124750 0.038258 00:53 添加softmax并查看输出结果 # # 获取验证集的预测 preds, targs = learn.get_preds(dl=learn.dls.valid) print(learn.dls.vocab) print(len(learn.dls.vocab)) [[\u0026lsquo;bacterial_leaf_blight\u0026rsquo;, \u0026lsquo;bacterial_leaf_streak\u0026rsquo;, \u0026lsquo;bacterial_panicle_blight\u0026rsquo;, \u0026lsquo;blast\u0026rsquo;, \u0026lsquo;brown_spot\u0026rsquo;, \u0026lsquo;dead_heart\u0026rsquo;, \u0026lsquo;downy_mildew\u0026rsquo;, \u0026lsquo;hispa\u0026rsquo;, \u0026rsquo;normal\u0026rsquo;, \u0026rsquo;tungro\u0026rsquo;], [\u0026lsquo;ADT45\u0026rsquo;, \u0026lsquo;AndraPonni\u0026rsquo;, \u0026lsquo;AtchayaPonni\u0026rsquo;, \u0026lsquo;IR20\u0026rsquo;, \u0026lsquo;KarnatakaPonni\u0026rsquo;, \u0026lsquo;Onthanel\u0026rsquo;, \u0026lsquo;Ponni\u0026rsquo;, \u0026lsquo;RR\u0026rsquo;, \u0026lsquo;Surya\u0026rsquo;, \u0026lsquo;Zonal\u0026rsquo;]] 2\n# 疾病类别有10个，品种类别也有10个 num_classes_disease = len(learn.dls.vocab[0]) # 10 num_classes_variety = len(learn.dls.vocab[1]) # 10 # 获取验证集的预测 preds, targs = learn.get_preds(dl=learn.dls.valid) # 划分疾病和品种的预测结果 disease_preds = preds[:, :num_classes_disease] # 获取第一部分，即疾病的预测 variety_preds = preds[:, num_classes_disease:] # 获取第二部分，即品种的预测 # 使用 softmax 计算概率 disease_probabilities = F.softmax(disease_preds, dim=1) variety_probabilities = F.softmax(variety_preds, dim=1) # 获取分类的最大概率对应的类别索引 predicted_disease_classes = disease_probabilities.argmax(dim=1) # 找出疾病类别的预测 predicted_variety_classes = variety_probabilities.argmax(dim=1) # 找出品种类别的预测 # 映射类别索引回实际标签 predicted_disease_labels = np.array(learn.dls.vocab[0])[predicted_disease_classes] predicted_variety_labels = np.array(learn.dls.vocab[1])[predicted_variety_classes] # 查看部分预测结果 results_df = pd.DataFrame({ \u0026#39;Predicted Disease\u0026#39;: predicted_disease_labels[:10], # 前10个的预测疾病 \u0026#39;Predicted Variety\u0026#39;: predicted_variety_labels[:10], # 前10个的预测品种 }) print(results_df) Predicted Disease Predicted Variety blast AndraPonni normal RR tungro ADT45 bacterial_leaf_streak KarnatakaPonni dead_heart ADT45 tungro ADT45 normal ADT45 normal ADT45 brown_spot ADT45 bacterial_leaf_blight ADT45 ","date":"27 March 2025","externalUrl":null,"permalink":"/posts/deeplearning/learning-based-paddydiseaseclassification/","section":"Posts","summary":"","title":"Learning Based Paddy_Disease_Classification","type":"posts"},{"content":"","date":"27 March 2025","externalUrl":null,"permalink":"/posts/","section":"Posts","summary":"","title":"Posts","type":"posts"},{"content":"","date":"27 March 2025","externalUrl":null,"permalink":"/series/","section":"Series","summary":"","title":"Series","type":"series"},{"content":"","date":"27 March 2025","externalUrl":null,"permalink":"/","section":"春日汀","summary":"","title":"春日汀","type":"page"},{"content":"","date":"27 March 2025","externalUrl":null,"permalink":"/categories/","section":"文章分类","summary":"","title":"文章分类","type":"categories"},{"content":"","date":"26 March 2025","externalUrl":null,"permalink":"/series/algorithm-learing/","section":"Series","summary":"","title":"Algorithm Learing","type":"series"},{"content":"","date":"26 March 2025","externalUrl":null,"permalink":"/categories/algorithm-problems/","section":"文章分类","summary":"","title":"Algorithm Problems","type":"categories"},{"content":" 哈希 # 本系列的大部分都是对labuladong和代码随想录网站的题目的练手汇总(部分是博主自己结合leetcode和洛谷题目的补充）初衷是博主自用学习哒!\n请所有阅读这系列博客的友友先移步阅览他们的网站，没有这些优秀网站的分享不会有这系列的文章，我只是知识的搬运工！\n两数之和 # 题目描述 # 给定一个整数数组 nums 和一个整数目标值 target，请你在该数组中找出 和为目标值 target 的那 两个 整数，并返回它们的数组下标。\n你可以假设每种输入只会对应一个答案，并且你不能使用两次相同的元素。\n你可以按任意顺序返回答案。\nAC代码 # class Solution { public: vector\u0026lt;int\u0026gt; twoSum(vector\u0026lt;int\u0026gt;\u0026amp; nums, int target) { unordered_map\u0026lt;int,int\u0026gt;map; for(int i=0;i\u0026lt;nums.size();i++) { int n=target-nums[i]; if(map.find(n)!=map.end())return {map.find(n)-\u0026gt;second,i}; else map.insert(pair\u0026lt;int,int\u0026gt;(nums[i],i)); } return {}; } }; P1102 A-B 数对 # 给出一串正整数数列以及一个正整数C, 要求计算出所有满足 A - B = C​ 的数对的个数（不同位置的数字一样的数对算不同的数对）。\n输入格式 # 输入共两行。\n第一行，两个正整数 N,C。\n第二行，N 个正整数，作为要求处理的那串数。\n输出格式 # 一行，表示该串正整数中包含的满足 A - B = C​ 的数对的个数。\n输入 # 4 1 1 1 2 3\n输出 # 3\n说明/提示 # 对于 75%​ 的数据，​1\u0026lt; N \u0026lt; 2000​。\n对于100%的数据，1 \u0026lt; N \u0026lt; 2×10^5​，0 \u0026lt; a_i \u0026lt; 2^30​，1 \u0026lt; C \u0026lt; 2^30​。\nAC代码 # #include \u0026lt;bits/stdc++.h\u0026gt; using namespace std; typedef long long ll; int main(){ int N, C; cin \u0026gt;\u0026gt; N \u0026gt;\u0026gt; C; vector\u0026lt;int\u0026gt; nums(N); unordered_map\u0026lt;int, int\u0026gt; freq; // 使用 int 作为频次类型 for(int i=0;i\u0026lt;N;i++){ int x; cin \u0026gt;\u0026gt; x; nums[i]=x; freq[x]++; } ll res = 0;//注意结果是long long 因为res会累加 for(const int \u0026amp;x : nums){ int target = x + C; if(freq.find(target) != freq.end()){ res += freq[target]; } } cout \u0026lt;\u0026lt; res; } ","date":"26 March 2025","externalUrl":null,"permalink":"/posts/algorithm/%E5%93%88%E5%B8%8C/","section":"Posts","summary":"","title":"哈希","type":"posts"},{"content":" 回溯 # 本系列的大部分都是对labuladong和代码随想录网站的题目的练手汇总(部分是博主自己结合leetcode和洛谷题目的补充）初衷是博主自用学习哒!\n请所有阅读这系列博客的友友先移步阅览他们的网站，没有这些优秀网站的分享不会有这系列的文章，我只是知识的搬运工！\n回溯三部曲\n1)确定回溯函数的返回值及参数\n每次回溯时要更新什么，要返回什么\n2）回溯函数的终止条件\n3）回溯搜索的遍历过程\nvector\u0026lt;vector\u0026gt; result\nvector path\nP4913 【深基16.例3】二叉树深度 # 题目描述 # 有一个 n(n\u0026lt;10^6)​ 个结点的二叉树。给出每个结点的两个子结点编号（均不超过 n），建立一棵二叉树（根节点的编号为 1​），如果是叶子结点，则输入 0 0。\n建好这棵二叉树之后，请求出它的深度。二叉树的深度是指从根节点到叶子结点时，最多经过了几层。\n输入格式 # 第一行一个整数 n​，表示结点数。\n之后 n行，第 i 行两个整数 l、r，分别表示结点 i 的左右子结点编号。若 l=0 则表示无左子结点，r=0 同理。\n输出格式 # 一个整数，表示最大结点深度。\n输入 # 7 2 7 3 6 4 5 0 0 0 0 0 0 0 0 输出 # 4 AC代码 # #include \u0026lt;iostream\u0026gt; #include \u0026lt;unordered_map\u0026gt; #include\u0026lt;math.h\u0026gt; using namespace std; unordered_map\u0026lt;int, pair\u0026lt;int, int\u0026gt;\u0026gt; tree; int maxDepth = 0; void traverse(int node, int currentDepth) { int left = tree[node].first; int right = tree[node].second; maxDepth = max(maxDepth,currentDepth); // 遍历左子树 if (left != 0) { traverse(left, currentDepth + 1); } // 遍历右子树 if (right != 0) { traverse(right, currentDepth + 1); } } int main() { int n; cin \u0026gt;\u0026gt; n; for (int i = 1; i \u0026lt;= n; ++i) { int l, r; cin \u0026gt;\u0026gt; l \u0026gt;\u0026gt; r; tree[i] = {l, r}; } traverse(1, 1); cout \u0026lt;\u0026lt; maxDepth \u0026lt;\u0026lt; endl; return 0; } P1025 [NOIP 2001 提高组] 数的划分 # 题目描述 # 将整数 n 分成 k份，且每份不能为空，任意两个方案不相同（不考虑顺序）。\n例如：n=7​，k=3，下面三种分法被认为是相同的。\n1,1,5​;\n1,5,1​;\n​5,1,1.\n问有多少种不同的分法。\n输入格式 # n,k（6\u0026lt;n\u0026lt; 200,2 \u0026lt;k \u0026lt;6）\n输出格式 # 1 个整数，即不同的分法。\n输入 # 7 3\n输出 # 4\n说明/提示 # 四种分法为：\n1,1,5;\n1,2,4;\n1,3,3;\n2,2,3.\nAC代码 # #include \u0026lt;bits/stdc++.h\u0026gt; using namespace std; int n, k, res = 0; stack\u0026lt;int\u0026gt; path; // 回溯函数 void backtrack(int start, int sum) { // 如果已经选择了k个数 if (path.size() == k) { if (sum == n) { res++; } return; } // 计算当前数最大可以是多少，避免无效递归 int remaining = k - path.size(); int max_i = (n - sum) / remaining; for(int i = start; i \u0026lt;= max_i; i++) { path.push(i); backtrack(i, sum + i); // 下一轮起始值至少为i，确保非递减!!!因为好几种算重复 path.pop(); } } int main(){ cin \u0026gt;\u0026gt; n \u0026gt;\u0026gt; k; backtrack(1, 0); cout \u0026lt;\u0026lt; res; return 0; } 未加入记忆数组，纯递归，超时：\n#include \u0026lt;iostream\u0026gt; using namespace std; int count(int num) { if (num == 0) return 0; int res = 1; // 包括自身作为一个数列的情况 for (int i = 1; i \u0026lt;= num / 2; ++i) { res += count(i); } return res; } int main() { int n; cin \u0026gt;\u0026gt; n; cout \u0026lt;\u0026lt; count(n) \u0026lt;\u0026lt; endl; return 0; } 改进：\n#include \u0026lt;iostream\u0026gt; using namespace std; int dp[100001] = {0}; // 记忆化数组 int count(int num) { if (num == 0) return 0; if (dp[num] != 0) return dp[num]; // 已计算过则直接返回 dp[num] = 1; // 初始化为1（自身） for (int i = 1; i \u0026lt;= num / 2; ++i) { dp[num] += count(i); } return dp[num]; } int main() { int n; cin \u0026gt;\u0026gt; n; cout \u0026lt;\u0026lt; count(n) \u0026lt;\u0026lt; endl; return 0; } #include \u0026lt;iostream\u0026gt; using namespace std; const int MAX_N = 100000; // 根据题目约束调整最大值 int dp[MAX_N + 1]; int main() { int n; cin \u0026gt;\u0026gt; n; dp[0] = 0; dp[1] = 1; for (int num = 2; num \u0026lt;= n; ++num) { dp[num] = 1; // 至少包含自己 for (int i = 1; i \u0026lt;= num / 2; ++i) { dp[num] += dp[i]; //求和过程 } } cout \u0026lt;\u0026lt; dp[n] \u0026lt;\u0026lt; endl; return 0; } 【蓝桥杯题库】81305. 小齐的字母方块拼写 # 问题描述 # 小齐有四个木块，每个木块是一个立方体，上面分别写有字母表的大写字母。她想通过排列这些木块，拼出一些单词。\n给定每个木块上的字母和小齐想拼出的单词列表，请确定她能成功拼出哪些单词。\n输入格式 # 第一行包含一个整数 N，表示小齐想拼出的单词数量。\n接下来的四行，每行包含一个字符串，表示一个木块上六个大写字母。\n接下来的 N行，每行包含一个小写字母数量在 1 到 4 之间的大写字母单词。\n输出格式 # 对于小齐想拼出的每个单词，如果她能成功拼出，输出 YES，否则输出 NO。\n样例输入 # 6 MOOOOO OOOOOO ABCDEF UVWXYZ COW MOO ZOO MOVE CODE FARM 样例输出 # YES NO YES YES NO NO 评测数据规模 # 1≤N≤10。\nAC代码 # #include \u0026lt;bits/stdc++.h\u0026gt; using namespace std; vector\u0026lt;string\u0026gt; table; vector\u0026lt;string\u0026gt; words; bool canSpell(const string\u0026amp; word, vector\u0026lt;bool\u0026gt;\u0026amp; used, int index) { if (index == word.size()) return true; for (int i = 0; i \u0026lt; 4; i++) { if (!used[i] \u0026amp;\u0026amp; table[i].find(word[index]) != string::npos) { //注意string也有find方法 != string::npos 和 unordered_map != map.end() 有点类似 used[i] = true; if (canSpell(word, used, index + 1)) return true; used[i] = false; } } return false; } int main() { int n; cin \u0026gt;\u0026gt; n; table.resize(4); words.resize(n); for (int i = 0; i \u0026lt; 4; i++)cin \u0026gt;\u0026gt; table[i]; for (int i = 0; i \u0026lt; n; i++) cin \u0026gt;\u0026gt; words[i]; for (int i = 0; i \u0026lt; n; i++) { vector\u0026lt;bool\u0026gt; used(4, false); if (canSpell(words[i], used, 0)) { cout \u0026lt;\u0026lt; \u0026#34;YES\u0026#34; \u0026lt;\u0026lt; endl; } else { cout \u0026lt;\u0026lt; \u0026#34;NO\u0026#34; \u0026lt;\u0026lt; endl; } } return 0; } ","date":"26 March 2025","externalUrl":null,"permalink":"/posts/algorithm/%E5%9B%9E%E6%BA%AF/","section":"Posts","summary":"","title":"回溯","type":"posts"},{"content":" 并查集 # 本系列的大部分都是对labuladong和代码随想录网站的题目的练手汇总(部分是博主自己结合leetcode和洛谷题目的补充）初衷是博主自用学习哒!\n请所有阅读这系列博客的友友先移步阅览他们的网站，没有这些优秀网站的分享不会有这系列的文章，我只是知识的搬运工！\n“并查集用于解决连通性问题”\ntemplate\njoin函数一定要先通过find函数寻根再进行关联\nint n = 1005; // n根据题目中节点数量而定，一般比节点数量大一点就好 vector\u0026lt;int\u0026gt; father (n, 0); // 并查集初始化 void init() { for (int i = 0; i \u0026lt; n; ++i) { father[i] = i; } } // 并查集里寻根的过程 int find(int u) { return u == father[u] ? u : father[u] = find(father[u]); // 路径压缩,将多层压缩为两层，只有第一层是根节点 } // 判断 u 和 v是否找到同一个根 bool isSame(int u, int v) { u = find(u); v = find(v); return u == v; } // 将v-\u0026gt;u 这条边加入并查集 void join(int u, int v) { u = find(u); // 寻找u的根 v = find(v); // 寻找v的根 if (u == v) return ; // 如果发现根相同，则说明在一个集合，不用两个节点相连直接返回 father[v] = u; } 删除冗余边Ⅱ（有向图） # 有一种有向树,该树只有一个根节点，所有其他节点都是该根节点的后继。该树除了根节点之外的每一个节点都有且只有一个父节点，而根节点没有父节点。有向树拥有 n 个节点和 n - 1 条边。输入一个有向图，该图由一个有着 n 个节点(节点编号 从 1 到 n)，n 条边，请返回一条可以删除的边，使得删除该条边之后该有向图可以被当作一颗有向树。\n输入描述\n第一行输入一个整数 N，表示有向图中节点和边的个数。\n后续 N 行，每行输入两个整数 s 和 t，代表这是 s 节点连接并指向 t 节点的单向边\n输出描述\n输出一条可以删除的边，若有多条边可以删除，请输出标准输入中最后出现的一条边。\n输入示例\n3 1 2 1 3 2 3 输出示例\n2 3\n题目分析 # 如果发现入度为2的节点，我们需要判断 删除哪一条边，删除后本图能成为有向树。如果是删哪个都可以，优先删顺序靠后的边。情况三： 如果没有入度为2的点，说明 图中有环了（注意是有向环）。\nisTreeAfterRemoveEdge() 判断删一个边之后是不是有向树： 将所有边的两端节点分别加入并查集，遇到要 要删除的边则跳过，如果遇到即将加入并查集的边的两端节点 本来就在并查集了，说明构成了环。\n如果顺利将所有边的两端节点（除了要删除的边）加入了并查集，则说明 删除该条边 还是一个有向树\ngetRemoveEdge()确定图中一定有了有向环，那么要找到需要删除的那条边： 将所有边的两端节点分别加入并查集，如果遇到即将加入并查集的边的两端节点 本来就在并查集了，说明构成了环。\nAC代码 # #include \u0026lt;iostream\u0026gt; #include \u0026lt;vector\u0026gt; using namespace std; int n; vector\u0026lt;int\u0026gt; father (1001, 0); // 并查集初始化 void init() { for (int i = 1; i \u0026lt;= n; ++i) { father[i] = i; } } // 并查集里寻根的过程 int find(int u) { return u == father[u] ? u : father[u] = find(father[u]); } // 将v-\u0026gt;u 这条边加入并查集 void join(int u, int v) { u = find(u); v = find(v); if (u == v) return ; father[v] = u; } // 判断 u 和 v是否找到同一个根 bool same(int u, int v) { u = find(u); v = find(v); return u == v; } // 在有向图里找到删除的那条边，使其变成树 void getRemoveEdge(const vector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt;\u0026amp; edges) { init(); // 初始化并查集 for (int i = 0; i \u0026lt; n; i++) { // 遍历所有的边 if (same(edges[i][0], edges[i][1])) { // 构成有向环了，就是要删除的边 cout \u0026lt;\u0026lt; edges[i][0] \u0026lt;\u0026lt; \u0026#34; \u0026#34; \u0026lt;\u0026lt; edges[i][1]; return; } else { join(edges[i][0], edges[i][1]); } } } // 删一条边之后判断是不是树 bool isTreeAfterRemoveEdge(const vector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt;\u0026amp; edges, int deleteEdge) { init(); // 初始化并查集 for (int i = 0; i \u0026lt; n; i++) { if (i == deleteEdge) continue; if (same(edges[i][0], edges[i][1])) { // 构成有向环了，一定不是树 return false; } join(edges[i][0], edges[i][1]); } return true; } int main() { int s, t; vector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt; edges; cin \u0026gt;\u0026gt; n; vector\u0026lt;int\u0026gt; inDegree(n + 1, 0); // 记录节点入度 for (int i = 0; i \u0026lt; n; i++) { cin \u0026gt;\u0026gt; s \u0026gt;\u0026gt; t; inDegree[t]++; edges.push_back({s, t}); } vector\u0026lt;int\u0026gt; vec; // 记录入度为2的边（如果有的话就两条边） // 找入度为2的节点所对应的边，注意要倒序，因为优先删除最后出现的一条边 for (int i = n - 1; i \u0026gt;= 0; i--) { if (inDegree[edges[i][1]] == 2) { vec.push_back(i); } } // 情况一、情况二 if (vec.size() \u0026gt; 0) { // 放在vec里的边已经按照倒叙放的，所以这里就优先删vec[0]这条边 if (isTreeAfterRemoveEdge(edges, vec[0])) { cout \u0026lt;\u0026lt; edges[vec[0]][0] \u0026lt;\u0026lt; \u0026#34; \u0026#34; \u0026lt;\u0026lt; edges[vec[0]][1]; } else { cout \u0026lt;\u0026lt; edges[vec[1]][0] \u0026lt;\u0026lt; \u0026#34; \u0026#34; \u0026lt;\u0026lt; edges[vec[1]][1]; } return 0; } // 处理情况三 // 明确没有入度为2的情况，那么一定有有向环，找到构成环的边返回就可以了 getRemoveEdge(edges); } 买云朵（01背包+并查集） # 题目描述 # Joe觉得云朵很美，决定去山上的商店买一些云朵。商店里有n朵云，云朵被编号为1，2，…，n，并且每朵云都有一个价值。但是商店老板跟他说，一些云朵要搭配来买才好，所以买一朵云则与这朵云有搭配的云都要买。\n但是Joe的钱有限，所以他希望买的价值越多越好。\n输入格式 # 第1行n，m，w，表示n朵云，m个搭配，Joe有w的钱。\n第 2-n+1 行，每行 ci，di 表示i朵云的价钱和价值。\n第n+2-n+1+m行，每行 ui，vi，表示买ui就必须买vi，同理，如果买vi就必须买ui。\n输出格式 # 一行，表示可以获得的最大价值。\n输入样例 # 5 3 10 3 10 3 10 3 10 5 100 10 1 1 3 3 2 4 2\n输出样例 # 1\n【提示】 【数据范围】\n30%的数据保证：n≤100；\n50%的数据保证：n≤1,000；m≤100；w≤1,000；\n100%的数据保证：n≤10,000；0≤m≤5000；w≤10,000。\nAC代码 # #include \u0026lt;bits/stdc++.h\u0026gt; using namespace std; const int MAXN = 10010; int dp[MAXN]; // DP数组，表示在钱为j时的最大价值 int father[MAXN]; // 并查集父节点 int cost[MAXN]; // 每组的成本 int value[MAXN]; // 每组的价值 vector\u0026lt;pair\u0026lt;int, int\u0026gt;\u0026gt; grid; // 存储每朵云的成本和价值 // 初始化并查集 void init(int n) { for (int i = 1; i \u0026lt;= n; i++) { father[i] = i; cost[i] = grid[i].first; value[i] = grid[i].second; } } // 查找并查集的根节点 int find(int x) { return x == father[x] ? x : father[x] = find(father[x]); } // 合并两个云朵的组 void join(int u, int v) { u = find(u); v = find(v); if (u == v) return; // 已经在同一组 father[v] = u; // 合并 cost[u] += cost[v]; // 合并成本 value[u] += value[v]; // 合并价值 } int main() { int n, m, w; cin \u0026gt;\u0026gt; n \u0026gt;\u0026gt; m \u0026gt;\u0026gt; w; grid.resize(n + 1); // 调整大小 for (int i = 1; i \u0026lt;= n; i++) { cin \u0026gt;\u0026gt; grid[i].first \u0026gt;\u0026gt; grid[i].second; } init(n); // 初始化并查集 // 处理搭配关系 for (int i = 1; i \u0026lt;= m; i++) { int u, v; cin \u0026gt;\u0026gt; u \u0026gt;\u0026gt; v; join(u, v); // 合并云朵的组 } // 动态规划求解 for (int i = 1; i \u0026lt;= n; i++) { if (father[i] == i) { // 只处理根节点 for (int j = w; j \u0026gt;= cost[i]; j--) { dp[j] = max(dp[j], dp[j - cost[i]] + value[i]); } } } // 输出最大价值 cout \u0026lt;\u0026lt; dp[w] \u0026lt;\u0026lt; endl; return 0; } 小齐的图论之旅 # 问题描述 # 小齐正在学习图论课程，并遇到了以下问题，她感到有些困惑。请你帮助她解决这个问题！\n给定一个连通的无向图，图中的顶点标号为 1…N，边标号为 1…M。对于图中的每个顶点 v，执行以下过程：\n令 S=v 且 h=0。\n当 ∣S∣\u0026lt;N 时：\n从所有与 S 中某一端点相连的边中，选择标号最小的边 e。将 e 的另一端点加入 S。\n更新h=10×h+e。\n返回 h mod 10^9+7。\n求解该过程的所有返回值。\n输入格式 # 第一行包含两个整数 N 和 M。\n接下来有M行，每行包含一条边 (a_e,b_e) 的两个端点，表示图中的一条边（1≤a_e\u0026lt;b_e≤N )。保证这些边构成一棵连通树，且每一对顶点之间最多只有一条边。\n输出格式 # 输出 N 行，其中第 i 行应包含从顶点 i 开始执行过程时的返回值。\n样例输入 # 3 2 1 2 2 3 样例输出 # 12 12 21 评测数据规模\n2≤N≤2×10^5， N−1≤M≤4×10^5。\nTLE代码 # //TLE了，仅作思路借鉴 #include \u0026lt;bits/stdc++.h\u0026gt; using namespace std; const int MOD = 1e9 + 7; struct Edge { int l, r; int val; }; struct CompareEdge { bool operator()(const Edge\u0026amp; a, const Edge\u0026amp; b) { return a.val \u0026gt; b.val; } }; int main() { int n, m; cin \u0026gt;\u0026gt; n \u0026gt;\u0026gt; m; vector\u0026lt;Edge\u0026gt; edges(m + 1, {0, 0, 0}); vector\u0026lt;vector\u0026lt;Edge\u0026gt;\u0026gt; adj(n + 1); for (int i = 1; i \u0026lt;= m; i++) { cin \u0026gt;\u0026gt; edges[i].l \u0026gt;\u0026gt; edges[i].r; edges[i].val = i; adj[edges[i].l].push_back(edges[i]); adj[edges[i].r].push_back(edges[i]); } vector\u0026lt;long long\u0026gt; result(n + 1, 0); for (int start = 1; start \u0026lt;= n; start++) { vector\u0026lt;bool\u0026gt; isinS(n + 1, false); isinS[start] = true; priority_queue\u0026lt;Edge, vector\u0026lt;Edge\u0026gt;, CompareEdge\u0026gt; pq; for (Edge\u0026amp; edge : adj[start]) { pq.push(edge); } int count = 1; long long h = 0; while (count \u0026lt; n \u0026amp;\u0026amp; !pq.empty()) { Edge edge = pq.top(); pq.pop(); int next = (isinS[edge.l]) ? edge.r : edge.l; if (isinS[next]) continue; isinS[next] = true; count++; h = (10 * h + edge.val) % MOD; for (Edge\u0026amp; edge : adj[next]) { if (!isinS[edge.l] || !isinS[edge.r]) { pq.push(edge); } } } result[start] = h; } for (int i = 1; i \u0026lt;= n; i++) { cout \u0026lt;\u0026lt; result[i] \u0026lt;\u0026lt; endl; } return 0; } ","date":"26 March 2025","externalUrl":null,"permalink":"/posts/algorithm/%E5%B9%B6%E6%9F%A5%E9%9B%86/","section":"Posts","summary":"","title":"并查集","type":"posts"},{"content":" 蓝桥杯C++题库2023-1 # 博主非ACMer，题目代码分析可能会有偏颇，建议谨慎参考博主的AC代码。分布这系列的帖子的初衷是分享官方的题库，以方便友友们备赛:)\n偶串 # 小蓝特别喜欢偶数，当他看到字符串时，他总数要检查一下是不是每种字符都是出现偶数次。给定一个字符串，请帮助小蓝检查一下该字符串是否满足要求。\n输入描述 # 输入一行包含一个字符串，由小写英文字母组成。\n输出描述 # 如果字符串中的每种字符都是出现偶数次，输出大写英文单词 YES ，否则输出大写英文单词 NO。\n样例输入 # banana 样例输出 # NO 评测用例规模 # 对于 50%的评测用例， 1≤ 字符串长度 ≤1000；\n对于所有评测用例，1≤ 字符串长度 ≤10^6 。\nAC代码 # 博主第一反应就是unordered_map，也就用它做出来了\n#include \u0026lt;bits/stdc++.h\u0026gt; using namespace std; int main() { unordered_map\u0026lt;char, int\u0026gt; map; string s; cin \u0026gt;\u0026gt; s; for (int i = 0; i \u0026lt; s.size(); i++) { map[s[i]]++; } for (auto\u0026amp; pair : map) { if (pair.second % 2 != 0) { cout \u0026lt;\u0026lt; \u0026#34;NO\u0026#34;; return 0; } } cout \u0026lt;\u0026lt; \u0026#34;YES\u0026#34;; return 0; } 划分 # 问题描述 # 给定 40个数，请将其任意划分成两组，每组至少一个元素。每组的权值为组内所有元素的和。划分的权值为两组权值的乘积。请问对于以下 40 个数，划分的权值最大为多少。\n问题分析 # 这个题先从二维数组分析，博主简单的画了个图，通过分析可以看出dp[i] [j] 是由​ dp[i-1] [j] ​和 dp[i] [j-nums [i]] 共同影响的。\n然后再压缩为一维数组解决。\n具体思路可参考代码随想录的0-1背包基础理论2,虽然是以0-1背包举例但是将二维数组压缩为一维数组的思路和注意的地方可以借鉴。\nAC代码 # 直觉就是分成两组和差不多的两部分，这样乘积最大。也就是说两部分的和尽量靠近sum/2\n采用动态规划，定义布尔数组dp[ i ] [ j ]，表示在前索引为0-i个数中，是否存在和为 j 的组合\n#include \u0026lt;iostream\u0026gt; #include \u0026lt;vector\u0026gt; #include \u0026lt;algorithm\u0026gt; using namespace std; int main() { int nums[40] = {5160, 9191, 6410, 4657, 7492, 1531, 8854, 1253, 4520, 9231, 1266, 4801, 3484, 4323, 5070, 1789, 2744, 5959, 9426, 4433, 4404, 5291, 2470, 8533, 7608, 2935, 8922, 5273, 8364, 8819, 7374, 8077, 5336, 8495, 5602, 6553, 3548, 5267, 9150, 3309}; int total_sum = 0; for (int i = 0; i \u0026lt; 40; i++) { total_sum += nums[i]; } vector\u0026lt;bool\u0026gt; dp(total_sum / 2 + 1, false); dp[0] = true; for (int i = 0; i \u0026lt; 40; i++) { for (int j = total_sum / 2; j \u0026gt;= nums[i]; j--) { if (dp[j - nums[i]]) { dp[j] = true; } } } long long max_product = 0; for (int i = total_sum / 2; i \u0026gt;= 0; i--) { if (dp[i]) { max_product = (long long)i * (total_sum - i); break; } } cout \u0026lt;\u0026lt; max_product \u0026lt;\u0026lt; endl; return 0; } 糖果分配 # 问题描述 # 两种糖果分别有9个和 16 个，要全部分给 7 个小朋友，每个小朋友得到的糖果总数最少为 2 个最多为 5 个，问有多少种不同的分法。糖果必须全部分完。\n只要有其中一个小朋友在两种方案中分到的糖果不完全相同，这两种方案就算作不同的方案。\nAC代码 # #include\u0026lt;bits/stdc++.h\u0026gt; using namespace std; int res=0; void dfs(int idx, int a_size, int b_size) { if (idx == 7) { if(a_size == 0 \u0026amp;\u0026amp; b_size == 0)res++; return; } for (int i = 0; i \u0026lt;= a_size; i++) { for (int j = 0; j \u0026lt;= b_size; j++) { if (i + j \u0026gt;= 2 \u0026amp;\u0026amp; i + j \u0026lt;= 5) { dfs(idx + 1, a_size - i, b_size - j); } } } } int main() { int a = 16, b = 9; dfs(0, a, b); cout \u0026lt;\u0026lt; res \u0026lt;\u0026lt; endl; return 0; } 混乘数字 # 问题描述 # 混乘数字的定义如下: 对于一个正整数 𝑛，如果存在正整数 𝑎,𝑏，使得𝑛=𝑎×𝑏，而且 𝑎和 𝑏的十进制数位中每个数字出现的次数之和与 𝑛中对应数字出现次数相同，则称 𝑛为混乘数字。例如，对于正整数 𝑛=126，存在 𝑎=6, 𝑏=21满足条件，因此126是一个混乘数字。又如，对于正整数 𝑛=180225，存在 𝑎=225, 𝑏=801 满足条件，因此 180225 是一个混乘数字。请你帮助计算出，1∼1000000(含)之间一共有多少个数字是混乘数字。\nAC代码 # #include \u0026lt;bits/stdc++.h\u0026gt; using namespace std; bool judge(int a, int b, int c, int* valid) { fill(valid, valid+10, 0); while(a){valid[a%10]++;a/=10;} while(b){valid[b%10]++;b/=10;} while(c){valid[c%10]--;c/=10;} for(int i=0;i\u0026lt;10;i++) if(valid[i]!=0) return false; return true; } int main() { const int n = 1e6; vector\u0026lt;bool\u0026gt; nums(n+1, false); int res = 0; for(int i=1; i\u0026lt;=n; i++){ for(int j=1; j\u0026lt;=i \u0026amp;\u0026amp; i*j\u0026lt;=n; j++){ int valid[10]={0}; if(judge(i,j,i*j,valid) \u0026amp;\u0026amp; !nums[i*j]){ nums[i*j] = true; //注意！一开始没加bool类型的nums数组，其实n是可能重复计数的 res++; } } } cout \u0026lt;\u0026lt; res; return 0; } 保险箱 # 问题描述 # 小蓝有一个保险箱，保险箱上共有 n 位数字。小蓝可以任意调整保险箱上的每个数字，每一次操作可以将其中一位增加 1 或减少 1。当某位原本为 9 或 0 时可能会向前（左边）进位/退位，当最高位（左边第一位）上的数字变化时向前的进位或退位忽略。\n示例：\n00000 的第 5 位减 1 变为 99999； 99999 的第 5 位减 1 变为 99998； 00000 的第 4 位减 1 变为 99990； 97993 的第 4 位加 1 变为 98003； 99909 的第 3 位加 1 变为 00009。 保险箱上一开始有一个数字 x，小蓝希望把它变成 y，这样才能打开它。问小蓝最少需要操作的次数。\n输入格式 # 第一行包含一个整数 n。 第二行包含一个 n 位整数 x。 第三行包含一个 n 位整数 y。 输出格式 # 输出一行包含一个整数表示答案。\n样例输入 # 5\n12349\n54321\n样例输出 # 11\n对于 30% 的评测用例，1 ≤ n ≤ 300； 对于 60% 的评测用例，1 ≤ n ≤ 3000； 对于所有评测用例，1 ≤ n ≤ 1e5，x 和 y 中仅包含数字 0 至 9，可能有前导零。 AC代码 # 这个状态方程没想出来:(\n定义动态规划状态 dp[i][j]，其中：\ni 表示当前处理到数字的第 i 位（从低位到高位，即从右到左）。 j 表示当前位的进位/退位状态： j = 0：第 i 位既没有进位也没有退位的操作数。 j = 1：第 i 位进行了进位（即当前位数字 +1 后影响了高位）的操作数。 j = 2：第 i 位进行了退位（即当前位数字 -1 后影响了高位）的操作数。 #include\u0026lt;bits/stdc++.h\u0026gt; using namespace std; int main() { int n; cin\u0026gt;\u0026gt;n; vector\u0026lt;int\u0026gt;xv(n,0),yv(n,0); vector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt;dp(n,vector\u0026lt;int\u0026gt;(3,0)); string x,y; cin\u0026gt;\u0026gt;x\u0026gt;\u0026gt;y; reverse(x.begin(),x.end()); reverse(y.begin(),y.end()); for(int i=0;i\u0026lt;n;i++) { xv[i]=x[i]-\u0026#39;0\u0026#39;; yv[i]=y[i]-\u0026#39;0\u0026#39;; } for(int i=0;i\u0026lt;n;i++) { if(i==0) { dp[i][0]=abs(xv[i]-yv[i]); dp[i][1]=yv[i]-xv[i]+10; dp[i][2]=xv[i]-yv[i]+10; } else { dp[i][0]=min({dp[i-1][0]+abs(yv[i]-xv[i]), dp[i-1][1]+abs(xv[i]+1-yv[i]), dp[i-1][2]+abs(xv[i]-1-yv[i])}); dp[i][1]=min({dp[i-1][0]+(10+yv[i]-xv[i]), dp[i-1][1]+(10+yv[i]-(xv[i]+1)), //上一位因为进位，因此xv[i]+1。又因为这一位要通过进位匹配，所以加上10 dp[i-1][2]+(10+yv[i]-(xv[i]-1))}); //以此类推 dp[i][2]=min({dp[i-1][0]+(10+xv[i]-yv[i]), dp[i-1][1]+(10+xv[i]+1-yv[i]), dp[i-1][2]+(10+xv[i]-1-yv[i])}); } } cout\u0026lt;\u0026lt;min({dp[n-1][0],dp[n-1][1],dp[n-1][2]}); return 0; } 管道 # 问题描述 # 有一根长度为 len 的横向管道，该管道按照单位长度分为 len 段，每一段的中央有一个可开关的阀门和一个检测水流的传感器。\n一开始管道是空的，位于 Li 的阀门会在 Si 时刻打开，并不断让水流入管道。\n对于位于 Li 的阀门，它流入的水在 Ti 时刻（Ti \u0026gt;= Si）会使得从第 Li - (Ti - Si) 段到第 Li + (Ti - Si) 段的传感器检测到水流。\n求管道中每一段中间的传感器都检测到有水流的最早时间。\n输入格式 # 输入的第一行包含两个整数 n, len，用一个空格分隔，分别表示会打开的阀门数和管道长度。\n接下来 n 行每行包含两个整数 Li, Si，用一个空格分隔，表示位于第 Li 段管道中央的阀门会在 Si 时刻打开。\n输出格式 # 输出一行包含一个整数表示答案。\n样例输入 # 3 10 1 1 6 5 10 2 样例输出 # 5 对于 30% 的评测用例，n \u0026lt;= 200，Si, len \u0026lt;= 3000； 对于 70% 的评测用例，n \u0026lt;= 5000，Si, len \u0026lt;= 10^5； 对于所有评测用例，1 \u0026lt;= n \u0026lt;= 10^5，1 \u0026lt;= Si, len \u0026lt;= 10^9，1 \u0026lt;= Li \u0026lt;= len，Li-1 \u0026lt; Li。 TLE代码 # （只通过了百分之60，没采用经典的区间覆盖套路）\n#include \u0026lt;bits/stdc++.h\u0026gt; using namespace std; bool judgeisfill(long long time, const vector\u0026lt;pair\u0026lt;long, long\u0026gt;\u0026gt;\u0026amp; water, long long len) { vector\u0026lt;bool\u0026gt; covered(len + 1, false); // 用于标记每一段是否被覆盖 for (int i = 0; i \u0026lt; water.size(); i++) { if (time \u0026gt;= water[i].second) { long long left = max(1LL, water[i].first - (time - water[i].second)); long long right = min(len, water[i].first + (time - water[i].second)); for (long long j = left; j \u0026lt;= right; j++) { covered[j] = true; // 标记覆盖范围 } } } for (int i = 1; i \u0026lt;= len; i++) { if (!covered[i]) return false; } return true; } int main() { int n; long long len; cin \u0026gt;\u0026gt; n \u0026gt;\u0026gt; len; vector\u0026lt;pair\u0026lt;long, long\u0026gt;\u0026gt; water(n, {0, 0}); vector\u0026lt;bool\u0026gt; pipe(n, false); for (int i = 0; i \u0026lt; n; i++) { cin \u0026gt;\u0026gt; water[i].first \u0026gt;\u0026gt; water[i].second; } long long s = 1e9; long long l = 1, r = s; while (l \u0026lt; r) { long long mid = (l + r) \u0026gt;\u0026gt; 1; if (judgeisfill(mid, water, len)) { r = mid; } else { l = mid + 1; } } cout \u0026lt;\u0026lt; l; return 0; } AC代码 # #include \u0026lt;bits/stdc++.h\u0026gt; using namespace std; bool judgeisfill(long long time, const vector\u0026lt;pair\u0026lt;long, long\u0026gt;\u0026gt;\u0026amp; water, long long len) { vector\u0026lt;pair\u0026lt;long long ,long long\u0026gt;\u0026gt; intervals; for (const auto\u0026amp; valve : water) { if (time \u0026gt;= valve.second) { // 如果当前时间大于等于阀门打开的时间 long long left = max(1LL, valve.first - (time - valve.second)); // 计算左边界 long long right = min(len, valve.first + (time - valve.second)); // 计算右边界 intervals.push_back({left, right}); } } if (intervals.empty()) return false; // 如果没有区间，直接返回 false // 按左端点排序 sort(intervals.begin(), intervals.end()); // 检查覆盖范围 long long r = intervals[0].second; // 初始最远右端点 if (intervals[0].first \u0026gt; 1) return false; // 如果第一个区间的左端点大于 1，直接返回 false for (size_t i = 1; i \u0026lt; intervals.size(); i++) { if (intervals[i].first \u0026gt; r + 1) return false; // 如果当前区间与 r 不相邻，返回 false r = max(r, intervals[i].second); // 更新最远右端点 } return r \u0026gt;= len; // 如果 r 大于等于 len，返回 true } int main() { int n; long long len; cin \u0026gt;\u0026gt; n \u0026gt;\u0026gt; len; vector\u0026lt;pair\u0026lt;long, long\u0026gt;\u0026gt; water(n); for (int i = 0; i \u0026lt; n; i++) { cin \u0026gt;\u0026gt; water[i].first \u0026gt;\u0026gt; water[i].second; } long long l = 0, r = 1e18; // 设置一个较大的上界 while (l \u0026lt; r) { long long mid = (l + r) \u0026gt;\u0026gt; 1; if (judgeisfill(mid, water, len)) { r = mid; } else { l = mid + 1; } } cout \u0026lt;\u0026lt; l \u0026lt;\u0026lt; endl; return 0; } 区间覆盖是一个经典问题。我们可以按区间的左端点来排序这些区间。我们检查这些区间是否覆盖了整个管道。如果第一个区间的左端点大于 1，那么表示管道的开始部分没有被覆盖，直接返回 false。否则我们设一个变量 r 表示可到达的最远距离，r的初始值为第一个区间的右端点。我们接着检查其他区间是否与 r相邻或重叠。如果当前区间和 r 相邻或重叠，我们将当前区间的右端点和 r取最大值。最后如果 r≥len则说明成功覆盖所有区间，否则说明没有。\n","date":"26 March 2025","externalUrl":null,"permalink":"/posts/lanqiaocup/lanqiaocup2023-1/","section":"Posts","summary":"","title":"Lanqiaocup2023-1","type":"posts"},{"content":"","date":"26 March 2025","externalUrl":null,"permalink":"/series/%E8%93%9D%E6%A1%A5%E6%9D%AF%E9%A2%98%E5%BA%93/","section":"Series","summary":"","title":"蓝桥杯题库","type":"series"},{"content":" Titanic 简洁的神经网络复现 # 本文复现的是fastai的官方视频教程 点击此处跳转\n其官方kaggle笔记本 点击此处跳转\n本篇博客是在fastai课程基础上进行总结，先复现只有一层隐藏层的神经网络，接着在此基础上复现深度学习简易框架\nimport torch import numpy as np import pandas as pd df = pd.read_csv(\u0026#34;./train.csv\u0026#34;) df.head() 处理缺失值（使用众数） # df.isna().sum() ##OUTPUT PassengerId 0 Survived 0 Pclass 0 Name 0 Sex 0 Age 177 SibSp 0 Parch 0 Ticket 0 Fare 0 Cabin 687 Embarked 2 dtype: int64 modes = df.mode().iloc[0] #iloc[0]是指的是选择第一个众数（即第一行），并将其赋值给 modes df.fillna(modes,inplace=True) df.head() 处理数值变量（长尾效应） # df.isna().sum() ##OUTPUT PassengerId 0 Survived 0 Pclass 0 Name 0 Sex 0 Age 0 SibSp 0 Parch 0 Ticket 0 Fare 0 Cabin 0 Embarked 0 dtype: int64 df.describe() PassengerId Survived Pclass Age SibSp Parch Fare count 891.000000 891.000000 891.000000 891.000000 891.000000 891.000000 891.000000 mean 446.000000 0.383838 2.308642 28.566970 0.523008 0.381594 32.204208 std 257.353842 0.486592 0.836071 13.199572 1.102743 0.806057 49.693429 min 1.000000 0.000000 1.000000 0.420000 0.000000 0.000000 0.000000 25% 223.500000 0.000000\u003e 2.000000 22.000000 0.000000 0.000000 7.910400 50% 446.000000 0.000000 3.000000 24.000000 0.000000 0.000000 14.454200 75% 668.500000 1.000000 3.000000 35.000000 1.000000 0.000000 31.000000 max 891.000000 1.000000 3.000000 80.000000 8.000000 6.000000 512.329200 df[\u0026#34;Fare\u0026#34;]=np.log(df[\u0026#34;Fare\u0026#34;]+1) 处理文本变量 # df.describe(include=[object]) Name Sex Ticket Cabin Embarked count 891 891 891 891 891 unique 891 2 681 147 3 top Dooley, Mr. Patrick male 1601 B96 B98 S freq 1 577 7 691 646 df = pd.get_dummies(df, columns=[\u0026#34;Sex\u0026#34;, \u0026#34;Pclass\u0026#34;, \u0026#34;Embarked\u0026#34;], dtype=int) df.columns ##OUTPUT Index([\u0026#39;PassengerId\u0026#39;, \u0026#39;Survived\u0026#39;, \u0026#39;Name\u0026#39;, \u0026#39;Age\u0026#39;, \u0026#39;SibSp\u0026#39;, \u0026#39;Parch\u0026#39;, \u0026#39;Ticket\u0026#39;, \u0026#39;Fare\u0026#39;, \u0026#39;Cabin\u0026#39;, \u0026#39;Sex_female\u0026#39;, \u0026#39;Sex_male\u0026#39;, \u0026#39;Pclass_1\u0026#39;, \u0026#39;Pclass_2\u0026#39;, \u0026#39;Pclass_3\u0026#39;, \u0026#39;Embarked_C\u0026#39;, \u0026#39;Embarked_Q\u0026#39;, \u0026#39;Embarked_S\u0026#39;], dtype=\u0026#39;object\u0026#39;) df.head() added_cols =[\u0026#34;Sex_male\u0026#34;,\u0026#34;Sex_female\u0026#34;,\u0026#34;Pclass_1\u0026#34;,\u0026#34;Pclass_2\u0026#34;,\u0026#34;Pclass_3\u0026#34;,\u0026#34;Embarked_C\u0026#34;,\u0026#34;Embarked_Q\u0026#34;,\u0026#34;Embarked_S\u0026#34;] df[added_cols].head( ) Sex_male Sex_female Pclass_1 Pclass_2 Pclass_3 Embarked_C Embarked_Q Embarked_S 0 1 0 0 0 1 0 0 1 1 0 1 1 0 0 1 0 0 2 0 1 0 0 1 0 0 1 3 0 1 1 0 0 0 0 1 4 1 0 0 0 1 0 0 1 划分数据集 # from torch import tensor indep_cols=[\u0026#34;Age\u0026#34;,\u0026#34;SibSp\u0026#34;,\u0026#34;Parch\u0026#34;,\u0026#34;Fare\u0026#34;]+added_cols t_dep = tensor(df.Survived) t_indep = tensor(df[indep_cols].values, dtype=torch.float) t_dep.shape torch.Size([891])\nt_indep.shape torch.Size([891, 12])\nt_indep ##OUTPUT tensor([[22., 1., 0., ..., 0., 0., 1.], [38., 1., 0., ..., 1., 0., 0.], [26., 0., 0., ..., 0., 0., 1.], ..., [24., 1., 2., ..., 0., 0., 1.], [26., 0., 0., ..., 1., 0., 0.], [32., 0., 0., ..., 0., 1., 0.]]) from fastai.data.transforms import RandomSplitter trn_split,val_split = RandomSplitter(seed=42)(df) trn_indep = t_indep[trn_split] val_indep = t_indep[val_split] trn_dep = t_dep[trn_split] val_dep = t_dep[val_split] len(trn_dep) 713\nlen(val_dep) 178\ntrn_dep = trn_dep[:,None] #升维，转成矩阵 vla_dep = trn_dep[:,None] 定义训练函数和验证函数 # 训练模型 # def train_model(epochs=30,lr=1.4): torch.manual_seed(442) #能复现训练过程 coeffs = init_coeffs() for i in range(epochs): one_epoch(coeffs,lr=lr) return coeffs def init_coeffs(n_hidden=20): layer1=(torch.rand(n_coeffs,n_hidden)-0.5)/n_hidden layer2=torch.rand(n_hidden,1)-0.3 const=torch.rand(1)[0] #[0]的作用是在张量中取标量 return layer1.requires_grad_(),layer2.requires_grad_(),const.requires_grad_() def one_epoch(coeffs,lr): loss = calc_loss(coeffs,trn_indep,trn_dep) loss.backward() with torch.no_grad():update_coeffs(coeffs,lr) print(f\u0026#34;{loss:.3f}\u0026#34;,end=\u0026#34;; \u0026#34;) def update_coeffs(coeffs,lr): for layer in coeffs: layer.sub_(layer.grad*lr) layer.grad.zero_() def calc_loss(coeffs,indeps,deps): return torch.abs(calc_preds(coeffs,indeps)-deps).mean() import torch.nn.functional as F def calc_preds(coeffs, indeps): l1,l2,const = coeffs res = F.relu(indeps@l1) res = res@l2 + const return torch.sigmoid(res) 验证模型 # def acc(coeffs): return(val_dep.bool()==(calc_preds(coeffs,val_indep)\u0026gt;0.5)).float().mean() 开始训练 # n_coeffs = trn_indep.shape[1] coeffs = train_model(lr=2.4) 0.579; 0.370; 0.369; 0.369; 0.368; 0.368; 0.367; 0.366; 0.366; 0.365; 0.365; 0.364; 0.363; 0.363; 0.362; 0.361; 0.361; 0.360; 0.359; 0.358; 0.357; 0.357; 0.356; 0.355; 0.354; 0.352; 0.351; 0.349; 0.347; 0.344;\nacc(coeffs) tensor(0.5322)\ncoeffs = train_model(lr=1.2) 0.579; 0.377; 0.370; 0.369; 0.369; 0.369; 0.368; 0.368; 0.368; 0.367; 0.367; 0.366; 0.366; 0.366; 0.365; 0.365; 0.365; 0.364; 0.364; 0.364; 0.363; 0.363; 0.363; 0.362; 0.362; 0.361; 0.361; 0.361; 0.360; 0.360;\nacc(coeffs) tensor(0.5869)\n深度学习 # def init_coeffs(): hiddens = [10, 10] sizes = [n_coeffs] + hiddens + [1] n = len(sizes) layers = [(torch.rand(sizes[i], sizes[i+1])-0.3)/sizes[i+1]*4 for i in range(n-1)] consts = [(torch.rand(1)[0]-0.5)*0.1 for i in range(n-1)] for l in layers+consts: l.requires_grad_() return layers,consts import torch.nn.functional as F def calc_preds(coeffs, indeps): layers,consts = coeffs n = len(layers) res = indeps for i,l in enumerate(layers): res = res@l + consts[i] if i!=n-1: res = F.relu(res) return torch.sigmoid(res) def update_coeffs(coeffs, lr): layers,consts = coeffs for layer in layers+consts: layer.sub_(layer.grad * lr) layer.grad.zero_() n_coeffs = trn_indep.shape[1] coeffs = train_model(lr=4) 0.373; 0.377; 0.377; 0.375; 0.371; 0.368; 0.365; 0.361; 0.357; 0.357; 0.356; 0.350; 0.344; 0.619; 0.340; 0.621; 0.621; 0.620; 0.618; 0.613; 0.569; 0.379; 0.379; 0.379; 0.379; 0.379; 0.379; 0.379; 0.379; 0.379;\nacc(coeffs) tensor(0.5955)\n","date":"24 March 2025","externalUrl":null,"permalink":"/posts/deeplearning/create-neural-net-from-scratch/","section":"Posts","summary":"","title":"Create Neural Net From Scratch","type":"posts"},{"content":"","externalUrl":null,"permalink":"/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":"","externalUrl":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"}]