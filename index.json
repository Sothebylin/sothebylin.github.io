
[{"content":"","date":"30 March 2025","externalUrl":null,"permalink":"/series/algorithm-learing/","section":"Series","summary":"","title":"Algorithm Learing","type":"series"},{"content":"","date":"30 March 2025","externalUrl":null,"permalink":"/categories/algorithm-problems/","section":"文章分类","summary":"","title":"Algorithm Problems","type":"categories"},{"content":"","date":"30 March 2025","externalUrl":null,"permalink":"/posts/","section":"Posts","summary":"","title":"Posts","type":"posts"},{"content":"","date":"30 March 2025","externalUrl":null,"permalink":"/series/","section":"Series","summary":"","title":"Series","type":"series"},{"content":"","date":"30 March 2025","externalUrl":null,"permalink":"/","section":"春日汀","summary":"","title":"春日汀","type":"page"},{"content":" 图论-2（Dijkstra\u0026amp;\u0026amp;Floyd) # 本系列的大部分都是对labuladong和代码随想录网站的题目的练手汇总(部分是博主自己结合leetcode和洛谷题目的补充）初衷是博主自用学习哒!\n请所有阅读这系列博客的友友先移步阅览他们的网站，没有这些优秀网站的分享不会有这系列的文章，我只是知识的搬运工！\nDijkstra三部曲(代码随想录) # 第一步，选源点到哪个节点近且该节点未被访问过 第二步，该最近节点被标记访问过 第三步，更新非访问节点到源点的距离（即更新minDist数组） minDist数组的含义：记录所有节点到起始点的最短路径\n#include \u0026lt;iostream\u0026gt; #include \u0026lt;vector\u0026gt; #include \u0026lt;climits\u0026gt; using namespace std; int main() { int n, m, p1, p2, val; cin \u0026gt;\u0026gt; n \u0026gt;\u0026gt; m; vector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt; grid(n + 1, vector\u0026lt;int\u0026gt;(n + 1, INT_MAX)); //gird记录各节点之间的权值 //初始化 for(int i = 0; i \u0026lt; m; i++){ cin \u0026gt;\u0026gt; p1 \u0026gt;\u0026gt; p2 \u0026gt;\u0026gt; val; grid[p1][p2] = val; } int start = 1; int end = n; // 存储从起始点到每个节点的最短距离 vector\u0026lt;int\u0026gt; minDist(n + 1, INT_MAX); // 记录顶点是否被访问过 vector\u0026lt;bool\u0026gt; visited(n + 1, false); minDist[start] = 0; // 起始点到自身的距离为0 for (int i = 1; i \u0026lt;= n; i++) { // 遍历所有节点 int minVal = INT_MAX; int cur = 1; //cur记录将要选取的最近的点 // 1、选距离源点最近且未访问过的节点，minval维护这些节点中与起始点的最小权值 for (int v = 1; v \u0026lt;= n; ++v) { if (!visited[v] \u0026amp;\u0026amp; minDist[v] \u0026lt; minVal) { minVal = minDist[v]; cur = v; } } visited[cur] = true; // 2、标记该节点已被访问 // 3、第三步，更新非访问节点到源点的距离（即更新minDist数组） for (int v = 1; v \u0026lt;= n; v++) { if (!visited[v] \u0026amp;\u0026amp; grid[cur][v] != INT_MAX \u0026amp;\u0026amp; minDist[cur] + grid[cur][v] \u0026lt; minDist[v]) { //需要判断INT_MAX,因为如果靠后面是否小于，相加会溢出 minDist[v] = minDist[cur] + grid[cur][v]; //每找到一个离起始点最近的且未被访问过的点，都要更新一下，以检查这个点是否会成为其它点最短路径的其中一个点 } } } if (minDist[end] == INT_MAX) cout \u0026lt;\u0026lt; -1 \u0026lt;\u0026lt; endl; // 不能到达终点 else cout \u0026lt;\u0026lt; minDist[end] \u0026lt;\u0026lt; endl; // 到达终点最短路径 } 如果题目要求把最短路的路径打印出来，则需要parent数组记录每次选取的cur节点\nfor (int v = 1; v \u0026lt;= n; v++) { if (!visited[v] \u0026amp;\u0026amp; grid[cur][v] != INT_MAX \u0026amp;\u0026amp; minDist[cur] + grid[cur][v] \u0026lt; minDist[v]) { minDist[v] = minDist[cur] + grid[cur][v]; parent[v] = cur; // 记录边，初始化为 vector\u0026lt;int\u0026gt; parent(n + 1, -1); } } 理解：\n每次都在找离起始点最近的未被访问点，不断更新其他的点离起始点的最短路径，当找不到找离起始点最近的未被访问点，程序结束。\n其与prim算法的区别是：prim是求 非访问节点到最小生成树的最小距离，而dijkstra是求 非访问节点到起始点的最小距离\nPrim算法和Dijkstra算法的区别 # 特性 Prim算法 Dijkstra算法 目标 找到一个图的最小生成树（Minimum Spanning Tree, MST） 找到从一个起始点到其他所有点的最短路径（Shortest Path） 适用场景 适用于无向图（通常用于连通图） 适用于有向图或无向图（通常用于带权图） 权值处理 权值可以是任意值（包括负权值），因为只关心边的总权重最小 不能处理负权边，否则可能导致错误结果 结果 生成一棵树，连接所有节点且总权重最小 生成一个最短路径树，表示从起点到其他节点的最短路径 Floyd(代码随想录) # 题目描述\n小明喜欢去公园散步，公园内布置了许多的景点，相互之间通过小路连接，小明希望在观看景点的同时，能够节省体力，走最短的路径。\n给定一个公园景点图，图中有 N 个景点（编号为 1 到 N），以及 M 条双向道路连接着这些景点。每条道路上行走的距离都是已知的。\n小明有 Q 个观景计划，每个计划都有一个起点 start 和一个终点 end，表示他想从景点 start 前往景点 end。由于小明希望节省体力，他想知道每个观景计划中从起点到终点的最短路径长度。 请你帮助小明计算出每个观景计划的最短路径长度。\n输入描述\n第一行包含两个整数 N, M, 分别表示景点的数量和道路的数量。\n接下来的 M 行，每行包含三个整数 u, v, w，表示景点 u 和景点 v 之间有一条长度为 w 的双向道路。\n接下里的一行包含一个整数 Q，表示观景计划的数量。\n接下来的 Q 行，每行包含两个整数 start, end，表示一个观景计划的起点和终点。\n输出描述\n对于每个观景计划，输出一行表示从起点到终点的最短路径长度。如果两个景点之间不存在路径，则输出 -1。\n【输入示例】\n7 3\n1 2 4\n2 5 6\n3 6 8\n2\n1 2\n2 3\n【输出示例】\n4\n-1\n【提示信息】\n从 1 到 2 的路径长度为 4，2 到 3 之间并没有道路。\n1 \u0026lt;= N, M, Q \u0026lt;= 1000.\n实质上是动态规划\ngrid[i][j][k] = m，表示节点i到节点j以[1...k] 集合中的一个节点为中间节点的最短距离为m。\n状态转移方程分两种情况：\n节点i到节点j的最短路径经过节点k 节点i 到节点j的最短路径不经过节点k 第一种情况，grid[i][j][k] = grid[i][k][k - 1] + grid[k][j][k - 1]\n节点i 到 节点k 的最短距离 是不经过节点k，中间节点集合为[1\u0026hellip;k-1]，所以 表示为grid[i][k][k - 1]\n节点k 到 节点j 的最短距离 也是不经过节点k，中间节点集合为[1\u0026hellip;k-1]，所以表示为 grid[k][j][k - 1]\n第二种情况，grid[i][j][k] = grid[i][j][k - 1]\n如果节点i 到 节点j的最短距离 不经过节点k，那么 中间节点集合[1\u0026hellip;k-1]，表示为 grid[i][j][k - 1]\n因为我们是求最短路，对于这两种情况自然是取最小值。\n即： grid[i][j][k] = min(grid[i][k][k - 1] + grid[k][j][k - 1]， grid[i][j][k - 1])\n对于理解这个动态转移方程，可以手算推演一下 #include \u0026lt;iostream\u0026gt; #include \u0026lt;vector\u0026gt; #include \u0026lt;list\u0026gt; using namespace std; int main() { int n, m, p1, p2, val; cin \u0026gt;\u0026gt; n \u0026gt;\u0026gt; m; vector\u0026lt;vector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt;\u0026gt; grid(n + 1, vector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt;(n + 1, vector\u0026lt;int\u0026gt;(n + 1, 10005))); // 因为边的最大距离是10^4 for(int i = 0; i \u0026lt; m; i++){ cin \u0026gt;\u0026gt; p1 \u0026gt;\u0026gt; p2 \u0026gt;\u0026gt; val; grid[p1][p2][0] = val; grid[p2][p1][0] = val; // 注意这里是双向图 } // 开始 floyd for (int k = 1; k \u0026lt;= n; k++) { for (int i = 1; i \u0026lt;= n; i++) { for (int j = 1; j \u0026lt;= n; j++) { grid[i][j][k] = min(grid[i][j][k-1], grid[i][k][k-1] + grid[k][j][k-1]); } } } // 输出结果 int z, start, end; cin \u0026gt;\u0026gt; z; while (z--) { cin \u0026gt;\u0026gt; start \u0026gt;\u0026gt; end; if (grid[start][end][n] == 10005) cout \u0026lt;\u0026lt; -1 \u0026lt;\u0026lt; endl; else cout \u0026lt;\u0026lt; grid[start][end][n] \u0026lt;\u0026lt; endl; } } ","date":"30 March 2025","externalUrl":null,"permalink":"/posts/algorithm/%E5%9B%BE%E8%AE%BA-2/","section":"Posts","summary":"","title":"图论-2","type":"posts"},{"content":"","date":"30 March 2025","externalUrl":null,"permalink":"/categories/","section":"文章分类","summary":"","title":"文章分类","type":"categories"},{"content":" 图论-1（prim\u0026amp;kruskal） # 本系列的大部分都是对labuladong和代码随想录网站的题目的练手汇总(部分是博主自己结合leetcode和洛谷题目的补充）初衷是博主自用学习哒!\n请所有阅读这系列博客的友友先移步阅览他们的网站，没有这些优秀网站的分享不会有这系列的文章，我只是知识的搬运工！\n“prim 算法是维护节点的集合，而 Kruskal 是维护边的集合。”\nprim三部曲(代码随想录) # prim算法是从节点的角度采用贪心的策略每次寻找距离最小生成树最近的节点并加入到最小生成树中。\n第一步，选距离生成树最近节点 （这里的生成树节点随便哪个节点都行） 第二步，最近节点加入生成树 第三步，更新非生成树节点到生成树的距离（即更新minDist数组） minDist数组用来记录每一个节点距离最小生成树的最近距离\n例题\n#include\u0026lt;iostream\u0026gt; #include\u0026lt;vector\u0026gt; #include \u0026lt;climits\u0026gt; using namespace std; int main() { int v, e; int x, y, k; cin \u0026gt;\u0026gt; v \u0026gt;\u0026gt; e; // 填一个默认最大值，题目描述val最大为10000 vector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt; grid(v + 1, vector\u0026lt;int\u0026gt;(v + 1, 10001)); while (e--) { cin \u0026gt;\u0026gt; x \u0026gt;\u0026gt; y \u0026gt;\u0026gt; k; // 因为是双向图，所以两个方向都要填上 grid[x][y] = k; grid[y][x] = k; } // 所有节点到最小生成树的最小距离 vector\u0026lt;int\u0026gt; minDist(v + 1, 10001); // 这个节点是否在树里 vector\u0026lt;bool\u0026gt; isInTree(v + 1, false); // 我们只需要循环 n-1次，建立 n - 1条边，就可以把n个节点的图连在一起 for (int i = 1; i \u0026lt; v; i++) { // 1、prim三部曲，第一步：选距离生成树最近节点 int cur = -1; // 选中哪个节点 加入最小生成树 int minVal = INT_MAX; for (int j = 1; j \u0026lt;= v; j++) { // 选取最小生成树节点的条件： // （1）不在最小生成树里 // （2）距离最小生成树最近的节点 if (!isInTree[j] \u0026amp;\u0026amp; minDist[j] \u0026lt; minVal) {//遍历非生成树的图，minVal维护最小权值 minVal = minDist[j]; cur = j; } } // 2、prim三部曲，第二步：最近节点（cur）加入生成树 isInTree[cur] = true; // 3、prim三部曲，第三步：更新非生成树节点到生成树的距离（即更新minDist数组） // 由于cur节点是新加入到最小生成树，那么只需要关心与 cur 相连的 非生成树节点 的距离 是否比 原来 非生成树节点到生成树节点的距离更小就能始终维护非生成树节点到树的最小距离 for (int j = 1; j \u0026lt;= v; j++) { if (!isInTree[j] \u0026amp;\u0026amp; grid[cur][j] \u0026lt; minDist[j]) {//注意理解，grid[cur][j] \u0026lt; minDist[j]，cur的加入是否改变了j到树的最小距离，若有则更新。 minDist[j] = grid[cur][j]; } } } // 统计结果 int result = 0; for (int i = 2; i \u0026lt;= v; i++) { // 不计第一个顶点，因为统计的是边的权值，v个节点有 v-1条边 result += minDist[i]; } cout \u0026lt;\u0026lt; result \u0026lt;\u0026lt; endl; } kruskal思路（代码随想录） # 边的权值排序，因为要优先选最小的边加入到生成树里\n遍历排序后的边\n如果边首尾的两个节点在同一个集合，说明如果连上这条边图中会出现环\n如果边首尾的两个节点不在同一个集合，加入到最小生成树，并把两个节点加入同一个集合\n#include \u0026lt;iostream\u0026gt; #include \u0026lt;vector\u0026gt; #include \u0026lt;algorithm\u0026gt; using namespace std; // l,r为 边两边的节点，val为边的数值 struct Edge { int l, r, val; }; // 节点数量 int n = 10001; // 并查集标记节点关系的数组 vector\u0026lt;int\u0026gt; father(n, -1); // 节点编号是从1开始的，n要大一些 // 并查集初始化 void init() { for (int i = 0; i \u0026lt; n; ++i) { father[i] = i; } } // 并查集的查找操作 int find(int u) { return u == father[u] ? u : father[u] = find(father[u]); // 路径压缩 } // 并查集的加入集合 void join(int u, int v) { u = find(u); // 寻找u的根 v = find(v); // 寻找v的根 if (u == v) return ; // 如果发现根相同，则说明在一个集合，不用两个节点相连直接返回 father[v] = u; } int main() { int v, e; int v1, v2, val; vector\u0026lt;Edge\u0026gt; edges; //不同点 int result_val = 0; cin \u0026gt;\u0026gt; v \u0026gt;\u0026gt; e; while (e--) { cin \u0026gt;\u0026gt; v1 \u0026gt;\u0026gt; v2 \u0026gt;\u0026gt; val; edges.push_back({v1, v2, val}); } // 执行Kruskal算法 // 按边的权值对边进行从小到大排序 sort(edges.begin(), edges.end(), [](const Edge\u0026amp; a, const Edge\u0026amp; b) { return a.val \u0026lt; b.val; }//匿名函数 //使用引用（\u0026amp;）可以避免拷贝，直接操作原始对象。 //使用const防止意外修改 ); // 并查集初始化 init(); // 从头开始遍历边 for (Edge edge : edges) { // 并查集，搜出两个节点的祖先 int x = find(edge.l); int y = find(edge.r); // 如果祖先不同，则不在同一个集合 if (x != y) { result_val += edge.val; // 这条边可以作为生成树的边 join(x, y); // 两个节点加入到同一个集合 } } cout \u0026lt;\u0026lt; result_val \u0026lt;\u0026lt; endl; return 0; } ","date":"30 March 2025","externalUrl":null,"permalink":"/posts/algorithm/%E5%9B%BE%E8%AE%BA-1/","section":"Posts","summary":"","title":"图论-1","type":"posts"},{"content":" 54461. 最大区间 # 问题描述 # 给定一个长度为 n的序列 Ai，求 L,R 使 (R−L+1)⋅min⁡(AL,AL+1,…,AR) 尽可能大，其中 min⁡ 表示最小值。\n你只需要输出最大的值即可，不需要输出具体的 L,R。\n输入格式 # 输入的第一行包含一个整数 n。\n第二行包含 n个整数，分别表示 A1,A2,…,An相邻两个整数之间使用一个空格分隔。\n输出格式 # 输出一行包含一个整数表示答案。\n样例输入 # 5 1 1 3 3 1 样例输出 # 6 对于 40%评测用例，1≤n≤5000，1≤Ai≤5000；\n对于所有评测用例，1≤n≤3×10^5， 1≤Ai≤10^9。\nAC代码 # #include\u0026lt;bits/stdc++.h\u0026gt; using namespace std; typedef long long ll; int main() { int n; cin \u0026gt;\u0026gt; n; vector\u0026lt;ll\u0026gt; nums(n, 0); for (int i = 0; i \u0026lt; n; i++) cin \u0026gt;\u0026gt; nums[i]; stack\u0026lt;ll\u0026gt; st; vector\u0026lt;ll\u0026gt; left(n, -1); // 左边第一个比它小的元素的位置 vector\u0026lt;ll\u0026gt; right(n, n); // 右边第一个比它小的元素的位置 // 计算 left for (int i = 0; i \u0026lt; n; i++) { while (!st.empty() \u0026amp;\u0026amp; nums[i] \u0026lt; nums[st.top()]) { right[st.top()] = i; st.pop(); } left[i] = st.empty() ? -1 : st.top(); //因为此时top停在的地方就是左边离nums[i]最近的大于等于其的索引 st.push(i); } // 计算 maxlen vector\u0026lt;ll\u0026gt; maxlen(n, 1); for (int i = 0; i \u0026lt; n; i++) { maxlen[i] = right[i] - left[i] - 1; } // 计算结果 ll res = 0; for (int i = 0; i \u0026lt; n; i++) { res = max(res, maxlen[i] * nums[i]); } cout \u0026lt;\u0026lt; res; return 0; } 48976.并石子 # 问题描述 # 在桌面从左至右横向摆放着 N堆石子。每一堆石子都有着相同的颜色，颜色可能是颜色 0，颜色 1 或者颜色 2 中的其中一种。\n现在要对石子进行合并，规定每次只能选择位置相邻并且颜色相同的两堆石子进行合并。合并后新堆的相对位置保持不变，新堆的石子数目为所选择的两堆石子数目之和，并且新堆石子的颜色也会发生循环式的变化。\n具体来说：两堆颜色 0的石子合并后的石子堆为颜色 1，两堆颜色 1的石子合并后的石子堆为颜色 2，两堆颜色 2的石子合并后的石子堆为颜色 0。本次合并的花费为所选择的两堆石子的数目之和。\n给出N堆石子以及他们的初始颜色，请问最少可以将它们合并为多少堆石子？如果有多种答案，选择其中合并总花费最小的一种，合并总花费指的是在所有的合并操作中产生的合并花费的总和。\n输入格式 # 第一行一个正整数N表示石子堆数。\n第二行包含N个用空格分隔的正整数，表示从左至右每一堆石子的数目。\n第三行包含 N个值为 0 或 1 或 2 的整数表示每堆石头的颜色。\n输出格式 # 一行包含两个整数，用空格分隔。其中第一个整数表示合并后数目最少的石头堆数，第二个整数表示对应的最小花费。\n样例输入 # 5 5 10 1 8 6 1 1 0 2 2 样例输出 # 2 44 样例说明 # 上图显示了两种不同的合并方式。其中节点中标明了每一堆的石子数目， 在方括号中标注了当前堆石子的颜色属性。左图的这种合并方式最终剩下了两堆石子，所产生的合并总花费为 15+14+15=44；右图的这种合并方式最终也剩下了两堆石子，但产生的合并总花费为 14+15+25=54。综上所述，我们选择合并花费为 44 的这种方式作为答案。\n对于 30% 的评测用例，1≤N≤10。\n对于 50% 的评测用例，1≤N≤50。\n对于 100% 的评测用例，1≤N≤300,1≤ 每堆石子的数目 ≤1000。\n官方题解 # //cpp #include \u0026lt;bits/stdc++.h\u0026gt; using namespace std; const int maxn = 303; const int inf = 1e9 + 5; int n, ans, dp[maxn][maxn][3], num[maxn][maxn], a[maxn], sum[maxn], c[maxn], cost[maxn][maxn]; int read() { int x; scanf(\u0026#34;%d\u0026#34;, \u0026amp;x); return x; } void solve() { n = read(); for (int i = 1; i \u0026lt;= n; i++) { for (int j = i; j \u0026lt;= n; j++) { num[i][j] = j - i + 1; for (int k = 0; k \u0026lt; 3; k++) { dp[i][j][k] = inf; } } } for (int i = 1; i \u0026lt;= n; i++) { a[i] = read(); sum[i] = a[i] + sum[i - 1]; } for (int i = 1; i \u0026lt;= n; i++) { c[i] = read(); dp[i][i][c[i]] = 0; } for (int len = 1; len \u0026lt;= n; len++) { for (int i = 1; i + len - 1 \u0026lt;= n; i++) { int j = i + len - 1; for (int col = 0; col \u0026lt; 3; col++) { int minn = inf; for (int k = i; k \u0026lt; j; k++) { if (dp[i][k][col] != inf \u0026amp;\u0026amp; dp[k + 1][j][col] != inf) { minn = min(minn, dp[i][k][col] + dp[k + 1][j][col]); } } if (minn == inf) { continue; } num[i][j] = 1; dp[i][j][(col + 1) % 3] = min(dp[i][j][(col + 1) % 3], minn + sum[j] - sum[i - 1]); } } } for (int i = 1; i \u0026lt;= n; i++) { for (int j = i; j \u0026lt;= n; j++) { if (num[i][j] == 1) { cost[i][j] = min(dp[i][j][0], min(dp[i][j][1], dp[i][j][2])); } } } for (int k = 1; k \u0026lt;= n; k++) { for (int i = 1; i \u0026lt;= k; i++) { for (int j = k + 1; j \u0026lt;= n; j++) { if (num[i][j] \u0026gt; num[i][k] + num[k + 1][j]) { num[i][j] = num[i][k] + num[k + 1][j]; cost[i][j] = cost[i][k] + cost[k + 1][j]; } else if (num[i][j] == num[i][k] + num[k + 1][j]) { cost[i][j] = min(cost[i][j], cost[i][k] + cost[k + 1][j]); } } } } printf(\u0026#34;%d %d\\n\u0026#34;, num[1][n], cost[1][n]); } int main() { solve(); return 0; } 48978.魔法阵 # 问题描述 # 样例输入 # 1 4 2 3 0 1 2 1 2 1 2 3 4\n样例输出 # 1 2\n样例输入 # 2 2 5 1 0 1 1\n样例输出 # 2 0\n样例说明 # 样例 1，存在路径：0→1→2→3，𝐾=2，如果在 0→1→2 上使用魔法，那么答案就是 0+0+4=4；如果在 1→2→3 上使用魔法，那么答案就是 2+0+0=2。再也找不到比 2 还小的答案了，所以答案就是 2。\n样例 2，存在路径：0→1→0→1→0→1，𝐾=5，这条路径总计恰好走了 5 条边，所以正好可以用魔法消除所有伤害，答案是 0。\n评测用例规模与约定 :\n对于 30% 的评测用例，1≤𝑁≤20。\n对于 50% 的评测用例，1≤𝑁≤100。\n对于 100% 的评测用例，1≤𝑁≤1000，1≤𝑀≤𝑁×(𝑁−1)2，1≤𝐾≤10，0≤𝑢,𝑣≤𝑁−1，1≤𝑤≤1000。\nAC代码 # 思路是BFS+动态规划+Dijskra+优先队列\ng：邻接表存储图（g[u] 存储 u 的所有邻接边 (v, w)）。 d[i][j]：表示从 0 走到 i 时，已经连续免除了 j 条边伤害的最小总伤害。 j = 0：表示未使用魔法。 j = 1：表示已经连续免除了 1 条边的伤害（正在积累 K 条边）。 j = K：表示已经成功使用魔法（免除了 K 条边的伤害）。\n#include\u0026lt;bits/stdc++.h\u0026gt; using namespace std; typedef pair\u0026lt;int, int\u0026gt; PII; const int INF = 1e9; int main() { int n, k, m; cin \u0026gt;\u0026gt; n \u0026gt;\u0026gt; k \u0026gt;\u0026gt; m; vector\u0026lt;vector\u0026lt;PII\u0026gt;\u0026gt; graph(n); while (m--) { int u, v, w; cin \u0026gt;\u0026gt; u \u0026gt;\u0026gt; v \u0026gt;\u0026gt; w; graph[u].push_back({v, w}); graph[v].push_back({u, w}); } // dist[i][j]: 到节点i时，已连续免除j条边的最小伤害 vector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt; dist(n, vector\u0026lt;int\u0026gt;(k + 1, INF)); dist[0][0] = 0; // 起点0，未使用魔法 // 优先队列：按当前伤害升序排列，这里的int值就是现在受到的伤害数 priority_queue\u0026lt;pair\u0026lt;int, PII\u0026gt;, vector\u0026lt;pair\u0026lt;int, PII\u0026gt;\u0026gt;, greater\u0026lt;pair\u0026lt;int, PII\u0026gt;\u0026gt;\u0026gt; pq; pq.push({0, {0, 0}}); // {当前总伤害, {节点, 已免除的边数}} while (!pq.empty()) { auto current = pq.top(); int total = current.first; int u = current.second.first; int j = current.second.second; pq.pop(); if (total \u0026gt; dist[u][j]) continue; // 已存在更优解，跳过 for (auto edge : graph[u]) { int v = edge.first; // 目标节点 int w = edge.second; // 边伤害 // 情况1：不使用魔法（j保持0） if (j == 0 \u0026amp;\u0026amp; dist[v][0] \u0026gt; total + w) { dist[v][0] = total + w; pq.push({dist[v][0], {v, 0}}); } // 情况2：正在积累魔法（j \u0026lt; k） if (j \u0026lt; k \u0026amp;\u0026amp; dist[v][j + 1] \u0026gt; total) { // 免除当前边的伤害 dist[v][j + 1] = total; pq.push({dist[v][j + 1], {v, j + 1}}); } // 情况3：魔法已用完（j == k） if (j == k \u0026amp;\u0026amp; dist[v][k] \u0026gt; total + w) { dist[v][k] = total + w; pq.push({dist[v][k], {v, k}}); } } } // 输出最小伤害：可能未使用魔法或已使用魔法 cout \u0026lt;\u0026lt; min(dist[n-1][0], dist[n-1][k]) \u0026lt;\u0026lt; endl; return 0; } 为什么用 BFS + DP？\n(1) 普通 BFS 的问题\nBFS 通常用于无权图的最短路径（所有边权重相同）。 但本题边有伤害值（权重），所以要用 Dijkstra（优先队列 BFS） 来保证每次取最小伤害的路径。 (2) 为什么还要 DP？\n可以选择 免除某些边的伤害，但最多连续免 K 条。 这意味着 到达同一个节点时，可能处于不同的“魔法状态”（比如已经免了 0 条、1 条、\u0026hellip;、K 条）。 因此，我们需要 额外维度 j 来记录当前免除了多少条边。 这里博主设置一个简单的例子来看看代码的逻辑\n","date":"29 March 2025","externalUrl":null,"permalink":"/posts/lanqiaocup/lanqiaocup2023-2/","section":"Posts","summary":"","title":"Lanqiaocup2023-2","type":"posts"},{"content":"","date":"29 March 2025","externalUrl":null,"permalink":"/series/%E8%93%9D%E6%A1%A5%E6%9D%AF%E9%A2%98%E5%BA%93/","section":"Series","summary":"","title":"蓝桥杯题库","type":"series"},{"content":" 单调栈 # 本系列的大部分都是对labuladong和代码随想录网站的题目的练手汇总(部分是博主自己结合leetcode和洛谷题目的补充）初衷是博主自用学习哒!\n请所有阅读这系列博客的友友先移步阅览他们的网站，没有这些优秀网站的分享不会有这系列的文章，我只是知识的搬运工！\n“一维数组，要寻找任一个元素的右边或者左边第一个比自己大或者小的元素的位置，就可以考虑使用单调栈“——代码随想录\n（二分也能实现，但是二分的前提是有序数组\n【Leetcode】例题—739每日温度 # 给定一个整数数组 temperatures ，表示每天的温度，返回一个数组 answer ，其中 answer[i] 是指对于第 i 天，下一个更高温度出现在几天后。如果气温在这之后都不会升高，请在该位置用 0 来代替。\n示例 1:\n输入: temperatures = [73,74,75,71,69,72,76,73] 输出: [1,1,4,2,1,1,0,0] 过程分析 # 模板代码 # 搬运的代码随想录的模板\n// 版本一 class Solution { public: vector\u0026lt;int\u0026gt; dailyTemperatures(vector\u0026lt;int\u0026gt;\u0026amp; T) { // 递增栈 stack\u0026lt;int\u0026gt; st; vector\u0026lt;int\u0026gt; result(T.size(), 0); st.push(0); for (int i = 1; i \u0026lt; T.size(); i++) { if (T[i] \u0026lt; T[st.top()]) { // 情况一 st.push(i); } else if (T[i] == T[st.top()]) { // 情况二 st.push(i); } else { while (!st.empty() \u0026amp;\u0026amp; T[i] \u0026gt; T[st.top()]) { // 情况三 result[st.top()] = i - st.top(); st.pop(); } st.push(i); } } return result; } }; 精简代码如下：\n// 版本二 class Solution { public: vector\u0026lt;int\u0026gt; dailyTemperatures(vector\u0026lt;int\u0026gt;\u0026amp; T) { stack\u0026lt;int\u0026gt; st; // 递增栈 vector\u0026lt;int\u0026gt; result(T.size(), 0); for (int i = 0; i \u0026lt; T.size(); i++) { while (!st.empty() \u0026amp;\u0026amp; T[i] \u0026gt; T[st.top()]) { // 注意栈不能为空 result[st.top()] = i - st.top(); st.pop(); } st.push(i); } return result; } }; 【蓝桥杯题库】54461. 最大区间 # 问题描述 # 给定一个长度为 n的序列 Ai，求 L,R 使 (R−L+1)⋅min⁡(AL,AL+1,…,AR) 尽可能大，其中 min⁡ 表示最小值。\n你只需要输出最大的值即可，不需要输出具体的 L,R。\n输入格式 # 输入的第一行包含一个整数 n。\n第二行包含 n个整数，分别表示 A1,A2,…,An相邻两个整数之间使用一个空格分隔。\n输出格式 # 输出一行包含一个整数表示答案。\n样例输入 # 5 1 1 3 3 1 样例输出 # 6 对于 40%评测用例，1≤n≤5000，1≤Ai≤5000；\n对于所有评测用例，1≤n≤3×10^5， 1≤Ai≤10^9。\n错误思路 # 一开始就是简单的想去套模板，但是这与模板题不同的是，其实并不是纯粹的单调，每次入栈其实是和栈底比。（那这样还不如用滑动窗口了感觉\u0026hellip;\n#include\u0026lt;bits/stdc++.h\u0026gt; using namespace std; typedef long long ll; int main() { int n; cin\u0026gt;\u0026gt;n; vector\u0026lt;ll\u0026gt;nums(n,0); for(int i=0;i\u0026lt;n;i++)cin\u0026gt;\u0026gt;nums[i]; stack\u0026lt;ll\u0026gt;st; vector\u0026lt;ll\u0026gt;maxlen(n,n); for(int i=0;i\u0026lt;n;i++) { while(!st.empty()\u0026amp;\u0026amp;nums[i]\u0026lt;st.top()) { maxlen[st.top()]=i-st.top(); st.pop(); } st.push(i); } ll res=0; for(int i=0;i\u0026lt;n;i++) { res=max(res,maxlen[i]*nums[i]); } cout\u0026lt;\u0026lt;res; return 0; } AC代码 # #include\u0026lt;bits/stdc++.h\u0026gt; using namespace std; typedef long long ll; int main() { int n; cin \u0026gt;\u0026gt; n; vector\u0026lt;ll\u0026gt; nums(n, 0); for (int i = 0; i \u0026lt; n; i++) cin \u0026gt;\u0026gt; nums[i]; stack\u0026lt;ll\u0026gt; st; vector\u0026lt;ll\u0026gt; left(n, -1); // 左边第一个比它小的元素的位置 vector\u0026lt;ll\u0026gt; right(n, n); // 右边第一个比它小的元素的位置 // 计算 left for (int i = 0; i \u0026lt; n; i++) { while (!st.empty() \u0026amp;\u0026amp; nums[i] \u0026lt; nums[st.top()]) { right[st.top()] = i; st.pop(); } left[i] = st.empty() ? -1 : st.top(); //因为此时top停在的地方就是左边离nums[i]最近的大于等于其的索引 st.push(i); } // 计算 maxlen vector\u0026lt;ll\u0026gt; maxlen(n, 1); for (int i = 0; i \u0026lt; n; i++) { maxlen[i] = right[i] - left[i] - 1; } // 计算结果 ll res = 0; for (int i = 0; i \u0026lt; n; i++) { res = max(res, maxlen[i] * nums[i]); } cout \u0026lt;\u0026lt; res; return 0; } 官方题解\n蓝桥杯官方是用的单调队列（但是它tag为什么是单调栈:(\n#include \u0026lt;bits/stdc++.h\u0026gt; using namespace std; typedef long long LL; int n; void solve() { int n; cin \u0026gt;\u0026gt; n; vector\u0026lt;LL\u0026gt; a(n), l(n, -1), r(n, n); for (int i = 0; i \u0026lt; n; ++i) { cin \u0026gt;\u0026gt; a[i]; } deque\u0026lt;int\u0026gt; s; for (int i = 0; i \u0026lt; n; ++i) { while (s.size() \u0026amp;\u0026amp; a[s.back()] \u0026gt;= a[i]) { s.pop_back(); } if (s.size()) l[i] = s.back(); s.push_back(i); } s.clear(); for (int i = n - 1; i \u0026gt;= 0; --i) { while (s.size() \u0026amp;\u0026amp; a[s.back()] \u0026gt;= a[i]) { s.pop_back(); } if (s.size()) r[i] = s.back(); s.push_back(i); } LL ans = 0; for (int i = 0; i \u0026lt; n; ++i) { ans = max(ans, 1LL * (r[i] - l[i] - 1) * a[i]); } cout \u0026lt;\u0026lt; ans \u0026lt;\u0026lt; \u0026#39;\\n\u0026#39;; } int main() { ios_base ::sync_with_stdio(false); cin.tie(0); cout \u0026lt;\u0026lt; setiosflags(ios::fixed) \u0026lt;\u0026lt; setprecision(2); int t = 1; while (t--) { solve(); } return 0; } ","date":"29 March 2025","externalUrl":null,"permalink":"/posts/algorithm/%E5%8D%95%E8%B0%83%E6%A0%88/","section":"Posts","summary":"","title":"单调栈\u0026单调队列","type":"posts"},{"content":"","date":"28 March 2025","externalUrl":null,"permalink":"/categories/deeplearning/","section":"文章分类","summary":"","title":"DeepLearning","type":"categories"},{"content":"","date":"28 March 2025","externalUrl":null,"permalink":"/series/fastai-course-learning-/","section":"Series","summary":"","title":"Fastai Course Learning ","type":"series"},{"content":" Learning pytorch based Collaborative Filtering # 本文复现的是fastai的官方视频教程 点击此处跳转\n其官方kaggle笔记本 点击此处跳转\n本文基于MovieLens的电影评分数据集，借助fastai的高级API，学习基于pytorch的神经网络建立\n创建DataLoaders # from fastai.collab import * from fastai.tabular.all import * set_seed(42) 因为博主是通过fastai这个pytorch的高级API进行学习的，因此获取数据可以直接从fastai里获取\npath = untar_data(URLs.ML_100k) movies = pd.read_csv( path/\u0026#39;u.item\u0026#39;, delimiter=\u0026#39;|\u0026#39;, encoding=\u0026#39;latin-1\u0026#39;, usecols=(0,1), names=(\u0026#39;movies\u0026#39;,\u0026#39;title\u0026#39;), header=None ) movies.head() movies title 0 1 Toy Story (1995) 1 2 GoldenEye (1995) 2 3 Four Rooms (1995) 3 4 Get Shorty (1995) 4 5 Copycat (1995) ratings = pd.read_csv( path/\u0026#39;u.data\u0026#39;, delimiter=\u0026#39;\\t\u0026#39;, header = None, names=[\u0026#39;user\u0026#39;,\u0026#39;movies\u0026#39;,\u0026#39;rating\u0026#39;,\u0026#39;timestamp\u0026#39;] ) ratings.head() user movies rating timestamp 0 196 242 3 881250949 1 186 302 3 891717742 2 22 377 1 878887116 3 244 51 2 880606923 4 166 346 1 886397596 下面构建用户-电影评分矩阵 ratings = ratings.merge(movies) ratings.head() user movies rating timestamp title 0 196 242 3 881250949 Kolya (1996) 1 186 302 3 891717742 L.A. Confidential (1997) 2 22 377 1 878887116 Heavyweights (1994) 3 244 51 2 880606923 Legends of the Fall (1994) 4 166 346 1 886397596 Jackie Brown (1997) ratings = ratings.drop(\u0026#39;timestamp\u0026#39;, axis=1) ratings.head() user movies rating title 0 196 242 3 Kolya (1996) 1 186 302 3 L.A. Confidential (1997) 2 22 377 1 Heavyweights (1994) 3 244 51 2 Legends of the Fall (1994) 4 166 346 1 Jackie Brown (1997) dls = CollabDataLoaders.from_df(ratings,item_name=\u0026#39;title\u0026#39;,bs=64) item_name='title'指定DataFrame中表示“项目”（item）的列名。\u0026rsquo;title\u0026rsquo; 是电影的标题列。\nbs=64指定每个批次（batch）的大小为 64。\ndls.show_batch() user title rating 0 782 Starship Troopers (1997) 2 1 943 Judge Dredd (1995) 3 2 758 Mission: Impossible (1996) 4 3 94 Farewell My Concubine (1993) 5 4 23 Psycho (1960) 4 5 296 Secrets \u0026amp; Lies (1996) 5 6 940 American President, The (1995) 4 7 334 Star Trek VI: The Undiscovered Country (1991) 1 8 380 Braveheart (1995) 4 9 690 So I Married an Axe Murderer (1993) 1 latent factor潜在因子 # Latent factor（潜在因子）是一种在机器学习和统计学中广泛使用的概念，尤其是在 协同过滤（Collaborative Filtering） 和 降维（Dimensionality Reduction） 领域。\n它指的是隐藏在数据背后、用于解释数据结构的不可观测的特征或变量。\n用户潜在因子（User Latent Factors）：每个用户可以用一个向量表示，这个向量捕捉了用户的偏好特征。例如，一个喜欢科幻电影的用户可能会在“科幻”维度上有较高的值。 物品潜在因子（Item Latent Factors）：每个物品（如电影、商品等）也可以用一个向量表示，这个向量捕捉了物品的特征。例如，一部科幻电影在“科幻”维度上会有较高的值。 通过这些潜在因子，模型可以预测用户对物品的评分或偏好。用户潜在因子和物品潜在因子的内积可以用来预测用户对物品的评分。\nn_users = len(dls.classes[\u0026#39;user\u0026#39;]) n_movies = len(dls.classes[\u0026#39;title\u0026#39;]) n_factors = 5 #显式定义潜在因子的数量 #先使用torch的random初始化 user_factors = torch.randn(n_users, n_factors) movie_factors = torch.randn(n_movies, n_factors) 这样就创建了潜在因子矩阵\n基于torch.nn.moudel自定义预测模块 # 创建一个新的模块类时，需要继承torch.nn.Module。并且在模块类中实现 forward方法，定义模块的前向传播逻辑。\n当调用模块时会自动调用forward，并将调用中的参数传递给它。\ntip:在FastAI中，Module是一个封装了 PyTorch 的 torch.nn.Module 的类，因此可以直接继承 Module 来定义自己的模型。\nclass DotProduct(Module): def __init__(self,n_users,n_movies,n_factors): self.user_factors = Embedding(n_users,n_factors) self.movie_factors = Embedding(n_movies, n_factors) def forward(self,x): users = self.user_factors(x[:,0]) movies = self.movie_factors(x[:,1]) return (users * movies).sum(dim=1) #向量的逐元素相乘后求和 嵌入层Embedding是一种将高维稀疏的独热编码向量映射到低维密集向量的方法。它通过学习一个嵌入矩阵Embedding Matrix，将每个类别映射到一个低维空间中的向量。\n嵌入层的工作方式是通过矩阵乘法来达到查找嵌入矩阵E中的行来获取嵌入向量的效果,这里创建两个嵌入矩阵user_factors 和 movie_factors分别表示用户和电影的潜在特征latent factors\n关于embedding是如何对应到索引的，博主举例画图来理解\nx,y = dls.one_batch() x.shape #output:\ntorch.Size([64, 2])\n创建学习模型 # model=DotProduct(n_users,n_movies,50) #这里定义潜在因子为50 learn = Learner(dls,model,loss_func=MSELossFlat()) Learner是FastAI中的核心类，用于封装模型训练和评估的逻辑。\n训练模型：learn.fit_one_cycle 或 learn.fit。 评估模型：learn.validate 或 learn.show_results。 调整学习率：learn.lr_find。 保存和加载模型：learn.save 和 learn.load。 MSELossFlat()是FastAI提供的一个封装的均方误差损失函数。它在内部处理了张量的展平flattening，使得损失函数可以直接应用于多维张量的输出。\n开始训练 # learn.fit_one_cycle(5, 5e-3) epoch train_loss valid_loss time 0 1.323817 1.340935 00:09 1 1.017078 1.092095 00:09 2 0.873056 0.974699 00:09 3 0.762055 0.897630 00:09 4 0.719645 0.874279 00:09 加入bias偏差、sigmoid函数 # class DotProductBias(Module): def __init__(self, n_users, n_movies, n_factors, y_range=(0,5.5)): self.user_factors = Embedding(n_users, n_factors) self.user_bias = Embedding(n_users, 1) self.movie_factors = Embedding(n_movies, n_factors) self.movie_bias = Embedding(n_movies, 1) self.y_range = y_range def forward(self, x): users = self.user_factors(x[:,0]) movies = self.movie_factors(x[:,1]) res = (users * movies).sum(dim=1, keepdim=True) res += self.user_bias(x[:,0]) + self.movie_bias(x[:,1]) return sigmoid_range(res, *self.y_range) 这里将y_range设置的是比5稍微大一点，因为sigmoid函数是没有到1的，而我们知道rating是可以到5的\nmodel = DotProductBias(n_users, n_movies, 50) learn = Learner(dls, model, loss_func=MSELossFlat()) learn.fit_one_cycle(5, 5e-3) epoch train_loss valid_loss time 0 0.873342 0.933685 00:10 1 0.580043 0.916336 00:10 2 0.412618 0.954554 00:09 3 0.312082 0.964509 00:10 4 0.309014 0.965015 00:10 训练效果并没有更好反而在验证集上的表现效果更糟了，注意到在验证集的损失一开始减少后面又增加了，猜测是过拟合的原因，下面引入权重衰减改进\n加入Weight Decay权重衰减进行训练 # 权重衰减Weight decay又称为L2正则化L2 regularization。\n它将所有权重平方和添加到损失函数中。当我们计算梯度时，它会为促使权重尽可能小。限制权重增长过快会阻碍模型的训练，但会使其泛化能力更强。\n将权重衰减wd添加到损失函数中如下：\nloss_with_wd = loss + wd * (parameters**2).sum()\n但实际上，计算这么大的数额并将其添加到损失中是非常低效的，而如果在计算梯度时加上就会高效很多\nparameters.grad += wd * 2 * parameters\n而在FastAi中，可以通过设置wd来实现添加权重衰减\nmodel = DotProductBias(n_users, n_movies, 50) learn = Learner(dls, model, loss_func=MSELossFlat()) learn.fit_one_cycle(5, 5e-3, wd=0.1) epoch train_loss valid_loss time 0 0.872865 0.944298 00:10 1 0.652497 0.890864 00:09 2 0.537503 0.877970 00:10 3 0.453741 0.863453 00:09 4 0.442338 0.859985 00:09 在验证集上的表现好很多了！\n基于torch.nn.moudel自定义嵌入层 # tip：Module中若想要将张量视为参数，我们必须将其包装在 nn.Parameter 类中。\ndef create_params(size): return nn.Parameter(torch.zeros(*size).normal_(0, 0.01)) class DotProductBias(Module): def __init__(self, n_users, n_movies, n_factors, y_range=(0,5.5)): self.user_factors = create_params([n_users, n_factors]) self.user_bias = create_params([n_users]) self.movie_factors = create_params([n_movies, n_factors]) self.movie_bias = create_params([n_movies]) self.y_range = y_range def forward(self, x): users = self.user_factors[x[:,0]] movies = self.movie_factors[x[:,1]] res = (users*movies).sum(dim=1) res += self.user_bias[x[:,0]] + self.movie_bias[x[:,1]] return sigmoid_range(res, *self.y_range) x 是一个二维张量，形状为 (batch_size, 2)，其中每一行包含一个用户 ID 和一个电影 ID。\nself.user_factors[x[:, 0]] 和 self.movie_factors[x[:, 1]] 分别获取用户和电影的潜在特征向量。self.user_factors[x[:, 0]] 会根据x[:, 0]从 self.user_factors 中提取对应的嵌入向量。结果是一个二维张量，形状为 [batch_size, n_factors]。\n(users * movies).sum(dim=1) 计算用户和电影潜在特征向量的点积。\nself.user_bias[x[:, 0]] 和 self.movie_bias[x[:, 1]] 分别获取用户和电影的偏置项。\nres += self.user_bias[x[:, 0]] + self.movie_bias[x[:, 1]] 将偏置项加到点积结果上。\nsigmoid_range(res, *self.y_range) 将结果映射到指定的范围内。\nmodel = DotProductBias(n_users, n_movies, 50) learn = Learner(dls, model, loss_func=MSELossFlat()) learn.fit_one_cycle(5, 5e-3, wd=0.1) epoch train_loss valid_loss time 0 0.889919 0.940679 00:10 1 0.666765 0.891549 00:09 2 0.534145 0.872604 00:09 3 0.447389 0.854790 00:09 4 0.429244 0.850999 00:09 和使用Embedding层的效果差不多\n偏差的含义 # movie_bias = learn.model.movie_bias.squeeze() idxs = movie_bias.argsort()[:5] [dls.classes[\u0026#39;title\u0026#39;][i] for i in idxs] squeeze() 是 PyTorch 的一个操作，用于去掉张量中大小为 1 的维度。如果 movie_bias 的形状是 [n_movies, 1]，squeeze() 会将其变为一维张量 [n_movies]。（但这好像不是必要的，create_params创建的就是一维的\nargsort() 是 PyTorch 的一个操作，返回张量中元素按值升序排列的索引。\n['Bio-Dome (1996)', 'Grease 2 (1982)', 'Showgirls (1995)', 'Spice World (1997)', 'Home Alone 3 (1997)'] 在这里展示的电影表示尽管电影很符合用户的潜在因子，但用户也可能不喜欢。\nidxs = movie_bias.argsort(descending=True)[:5] [dls.classes[\u0026#39;title\u0026#39;][i] for i in idxs] ['Titanic (1997)', 'Shawshank Redemption, The (1994)', \u0026quot;Schindler's List (1993)\u0026quot;, 'Rear Window (1954)', 'L.A. Confidential (1997)'] 在这里展示的电影表示尽管电影 不符合用户的潜在因子，用户也可能喜欢\n（titanic经典电影是有原因的！\n使用PCA解释嵌入矩阵 # 主成分分析 (PCA)能提取嵌入矩阵中最重要的潜在方向，对电影嵌入向量进行PCA降维，并将降维后的结果可视化:\ng = ratings.groupby(\u0026#39;title\u0026#39;)[\u0026#39;rating\u0026#39;].count() top_movies = g.sort_values(ascending=False).index.values[:1000] top_idxs = tensor([learn.dls.classes[\u0026#39;title\u0026#39;].o2i[m] for m in top_movies]) movie_w = learn.model.movie_factors[top_idxs].cpu().detach() movie_pca = movie_w.pca(3) fac0,fac1,fac2 = movie_pca.t() idxs = list(range(50)) X = fac0[idxs] Y = fac2[idxs] plt.figure(figsize=(12,12)) plt.scatter(X, Y) for i, x, y in zip(top_movies[idxs], X, Y): plt.text(x,y,i, color=np.random.rand(3)*0.7, fontsize=11) plt.show() 点的位置表示电影在主成分上的值。点越靠近某个方向，表示该电影在该主成分上的值越大。 如果某些点聚集在一起，表示这些电影在主成分上的值相似，可能具有相似的特征(好像挺分散的 使用fast.ai实现 # learn = collab_learner(dls, n_factors=50, y_range=(0, 5.5)) learn.fit_one_cycle(5, 5e-3, wd=0.1) 参数顺序分别是：epochs 参数、lr_max 参数以及wd权重衰减的缩放因子\nepoch train_loss valid_loss time 0 0.888463 0.945857 00:09 1 0.684889 0.887451 00:09 2 0.521428 0.868976 00:10 3 0.458419 0.854433 00:09 4 0.441765 0.849997 00:09 展示模型每层的详细系数 learn.model EmbeddingDotBias(\n(u_weight): Embedding(944, 50)\n(i_weight): Embedding(1665, 50)\n(u_bias): Embedding(944, 1)\n(i_bias): Embedding(1665, 1) )\n深度学习实现协同过滤 # embs = get_emb_sz(dls) embs [(944, 74), (1665, 102)]\nfast.ai的get_emb_sz通过启发式搜索能返回推荐设置的嵌入层大小\nclass CollabNN(Module): def __init__(self, user_sz, item_sz, y_range=(0,5.5), n_act=100): self.user_factors = Embedding(*user_sz) self.item_factors = Embedding(*item_sz) self.layers = nn.Sequential( nn.Linear(user_sz[1]+item_sz[1], n_act), nn.ReLU(), nn.Linear(n_act, 1)) self.y_range = y_range def forward(self, x): embs = self.user_factors(x[:,0]),self.item_factors(x[:,1]) x = self.layers(torch.cat(embs, dim=1))#将嵌入与激活连接在一起 return sigmoid_range(x, *self.y_range) model = CollabNN(*embs) learn = Learner(dls, model, loss_func=MSELossFlat()) learn.fit_one_cycle(5, 5e-3, wd=0.01) epoch train_loss valid_loss time 0 0.881015 0.971740 00:11 1 0.860780 0.892812 00:11 2 0.825322 0.880496 00:11 3 0.760412 0.866387 00:12 4 0.757029 0.866201 00:11 通过设置use_nn为true参数能轻松创建更多层。例如，可以在这里分别创建两个大小为 100 和 50 的隐藏层\nlearn = collab_learner(dls, use_nn=True, y_range=(0, 5.5), layers=[100,50]) learn.fit_one_cycle(5, 5e-3, wd=0.1) epoch train_loss valid_loss time 0 0.966844 0.984823 00:13 1 0.905129 0.915699 00:13 2 0.831309 0.894061 00:13 3 0.808309 0.867972 00:13 4 0.741418 0.864273 00:13 ","date":"28 March 2025","externalUrl":null,"permalink":"/posts/deeplearning/learning-pytorch-based-collaborative-filtering/","section":"Posts","summary":"","title":"Learning Pytorch Based Collaborative Filtering","type":"posts"},{"content":" Learning neural network further # 本文复现的是fastai的官方视频教程 点击此处跳转\n其官方kaggle笔记本 点击此处跳转\n本篇博客是在fastai课程基础上进行总结，基于kaggle上的paddy disease classificatin比赛，使用了Resnet框架和Convnext框架实现对疾病的分类，结果发现Convnext比Resnext效果更好，并且使用了测试时间增强（TTA）、扩大训练规模、对图像进行填充等方式探究降低误差的效果，最后还构建了多输出模型，输出疾病以及种类。\n准备阶段 # !pip install -Uq fastkaggle from fastai import * from fastkaggle import * comp = \u0026#39;paddy-disease-classification\u0026#39; path = setup_comp(comp, install=\u0026#39;fastai \u0026#34;timm\u0026gt;=0.6.2.dev0\u0026#34;\u0026#39;) path.ls() (#4) [Path(\u0026rsquo;../input/paddy-disease-classification/sample_submission.csv\u0026rsquo;),Path(\u0026rsquo;../input/paddy-disease-classification/train_images\u0026rsquo;),Path(\u0026rsquo;../input/paddy-disease-classification/train.csv\u0026rsquo;),Path(\u0026rsquo;../input/paddy-disease-classification/test_images\u0026rsquo;)]\nfrom fastai.vision.all import * trn_path = path/\u0026#39;train_images\u0026#39; imgfiles = get_image_files(trn_path) img = PILImage.create(imgfiles[0]) print(img.size) img.to_thumb(128) (480, 640)\n不过要注意的是PIL展示的图像尺寸格式\n库/方法 尺寸格式 备注 PIL / PILImage (宽度, 高度) 直接对应图像的实际像素尺寸 OpenCV (高度, 宽度) OpenCV 使用 (h, w) 的 NumPy 风格 matplotlib (宽度, 高度) 与 PIL 一致 PyTorch (通道, 高度, 宽度) 张量格式（需额外注意） 为检查所有图像的尺寸采用fastcore中的parallel模块并行处理\nfrom fastcore.parallel import * def f(o): return PILImage.create(o).size sizes = parallel(f, imgfiles, n_workers=8) pd.Series(sizes).value_counts() (480, 640) 10403 (640, 480) 4 Name: count, dtype: int64\ndls = ImageDataLoaders.from_folder( trn_path, valid_pct=0.2, #当未划分训练集和验证集时需要传递valid_pct参数 seed=42, item_tfms=Resize(480,method=\u0026#39;squish\u0026#39;), batch_tfms=aug_transforms(size=128,min_scale=0.75) ) item_tfms 是 ImageDataLoaders.from_folder() 方法的一个参数名，全称为 ​item transformations​（单样本变换）。它用于定义在数据加载时对单个图像（item）应用的预处理或增强操作。\ndls.show_batch(max_n=6) fastai 的 from_folder 方式能从子文件夹名自动提取类别。\n采用Resnet26d训练 # 定义学习模型并查找学习率 # learn = vision_learner( dls, \u0026#39;resnet26d\u0026#39;, metrics=error_rate, path=\u0026#39;.\u0026#39;).to_fp16() model.safetensors: 0%| | 0.00/64.2M [00:00\u0026lt;?, ?B/s]\n参数 作用 示例值/类型 补充说明 dls 数据加载器，包含训练集和验证集 ImageDataLoaders对象 需通过 DataBlock 或工厂函数（如 ImageDataLoaders.from_folder）生成 'resnet26d' 模型架构（ResNet的变体，平衡速度和精度） 字符串或nn.Module 其他选项：'resnet34', 'efficientnet_b0', 'convnext_tiny' 等 metrics 训练时计算的评估指标（支持单指标或列表） error_rate 常用指标：\n• accuracy\n• Precision\n• Recall\n• F1Score\n• RocAuc（二分类）\n• Perplexity（语言模型） path 模型和日志的保存目录（.表示当前目录） 字符串或Path对象 保存内容包括：\n• 模型参数（.pth）\n• 训练日志（history.csv） .to_fp16() 启用混合精度训练（FP16），加速训练并减少显存占用 方法调用 需GPU支持（如NVIDIA Volta+架构）\n可用 .to_fp32() 强制禁用 查找一下最佳的学习率\nlearn.lr_find(suggest_funcs=(valley,slide)) SuggestedLRs(valley=0.0010000000474974513, slide=0.0020892962347716093)\nsuggest_funcs 参数：指定学习率推荐策略\nvalley：选择损失下降最陡峭的点（避免过大的学习率）。\nslide：选择损失开始平稳上升的点（保守但稳定）。\nResnet26d模型训练 # learn.fine_tune(3,0.01) epoch train_loss valid_loss error_rate time 0 1.783480 1.293824 0.423835 00:42 epoch train_loss valid_loss error_rate time 0 1.128158 0.715136 0.238827 00:42 1 0.802069 0.490330 0.160980 00:42 2 0.569914 0.385769 0.135992 00:42 fine_tune() 是 fastai 提供的微调方法，分两阶段自动训练：\n1)冻结阶段（Freeze）​\n仅训练新增的头部层​（替换后的最后一层），预训练主干层保持冻结。 默认1轮​（可通过 freeze_epochs 修改）。 学习率自动设为 base_lr/10（若未指定 base_lr，则用传入的 0.01）。\n2)解冻阶段（Unfreeze）​\n解冻所有层，全部参与训练。 训练剩余轮数​（此处 3 - 1 = 2 轮）。 使用传入的学习率 0.01（或通过 lr_find() 优化的值。\nfastai 的 vision_learner 会自动从数据加载器 (dls) 中推断输出维度\n新增的头部层（New Head Layers）​ 是指当使用预训练模型（如ResNet、EfficientNet等）进行迁移学习时，​替换或添加的最后一层（或几层）网络结构，用于适配当前任务的输出需求\n将训练好的Resnet26d应用到测试集上 # tst_files = get_image_files(path/\u0026#39;test_images\u0026#39;).sorted() tst_dl = dls.test_dl(tst_files) test_dl() 是 ​fastai 库中 DataLoaders 对象的一个方法，专门用于为测试集（或推理数据）​创建一个与训练时数据预处理一致的数据加载器\nprobs,_,idxs = learn.get_preds(dl=tst_dl, with_decoded=True) idxs tensor([7, 8, 4, \u0026hellip;, 8, 1, 5])\nmapping = dict(enumerate(dls.vocab)) results = pd.Series(idxs.numpy(), name=\u0026#34;idxs\u0026#34;).map(mapping) results 0 hispa\n1 normal\n2 brown_spot\n3 blast\n4 blast\n​ \u0026hellip; 3464 dead_heart\n3465 hispa\n3466 normal\n3467 bacterial_leaf_streak\n3468 dead_heart\nName: idxs, Length: 3469, dtype: object\nss = pd.read_csv(path/\u0026#39;sample_submission.csv\u0026#39;) ss[\u0026#39;label\u0026#39;] = results ss.to_csv(\u0026#39;subm.csv\u0026#39;, index=False) !head subm.csv image_id, label\n200001.jpg, hispa\n200002.jpg, normal\n200003.jpg, brown_spot\n200004.jpg, blast\n200005.jpg, blast\n200006.jpg, brown_spot\n200007.jpg, dead_heart\n200008.jpg, brown_spot\n200009.jpg, hispa\n改进一下Resnet26d # 为了让模型训练的更快，我们可以将图像的大小缩小（让像素数量减少4倍）\ntrn_path = Path(\u0026#39;sml\u0026#39;) resize_images(path/\u0026#39;train_images\u0026#39;, dest=trn_path, max_size=256, recurse=True) fastai 有一个函数可以做到这一点，同时保持数据的文件夹结构即resize_images\ndls = ImageDataLoaders.from_folder(trn_path, valid_pct=0.2, seed=42, item_tfms=Resize((256,192))) dls.show_batch(max_n=3) 将之前训练的步骤封装起来\ndef train(arch, item, batch, epochs=5): dls = ImageDataLoaders.from_folder(trn_path, seed=42, valid_pct=0.2, item_tfms=item, batch_tfms=batch) learn = vision_learner(dls, arch, metrics=error_rate).to_fp16() learn.fine_tune(epochs, 0.01) return learn learn = train(\u0026#39;resnet26d\u0026#39;, item=Resize(192), batch=aug_transforms(size=128, min_scale=0.75)) epoch train_loss valid_loss error_rate time 0 1.942248 1.426470 0.463239 00:17 epoch train_loss valid_loss error_rate time 0 1.289961 0.999535 0.333974 00:19 1 1.018504 0.688591 0.216242 00:18 2 0.740462 0.482486 0.160019 00:18 3 0.529927 0.406155 0.124459 00:18 4 0.435775 0.372298 0.116771 00:18 可以看到准确率是有了提升\n使用ConNeXt_small模型 # 定义convnext_small_in22k学习模型 # arch = \u0026#39;convnext_small_in22k\u0026#39; learn = train(arch, item=Resize(192, method=\u0026#39;squish\u0026#39;), batch=aug_transforms(size=128, min_scale=0.75)) epoch train_loss valid_loss error_rate time 0 1.245993 0.795927 0.255166 00:55 epoch train_loss valid_loss error_rate time 0 0.647190 0.434121 0.148967 00:53 1 0.477503 0.379062 0.119654 00:53 2 0.300822 0.191345 0.060548 00:52 3 0.192040 0.134807 0.039404 00:52 4 0.127018 0.125794 0.036521 00:52 损失降低了很多，Convnext模型在这个数据集上的效果要比Resnet好\n对图像进行填充处理 # dls = ImageDataLoaders.from_folder(trn_path, valid_pct=0.2, seed=42, item_tfms=Resize(192, method=ResizeMethod.Pad, pad_mode=PadMode.Zeros)) dls.show_batch(max_n=3) 再次进行训练 # learn = train(arch, item=Resize((256,192), method=ResizeMethod.Pad, pad_mode=PadMode.Zeros), batch=aug_transforms(size=(171,128), min_scale=0.75)) epoch train_loss valid_loss error_rate time 0 1.225350 0.838977 0.261413 00:44 epoch train_loss valid_loss error_rate time 0 0.647279 0.475882 0.153292 00:51 1 0.532679 0.375510 0.116290 00:51 2 0.348923 0.203416 0.067275 00:50 3 0.207750 0.132006 0.037001 00:50 4 0.140231 0.124632 0.035079 00:50 好像效果也没很大的提升:(\n应用测试时间增强Test time augmentation（TTA） # During inference or validation, creating multiple versions of each image, using data augmentation, and then taking the average or maximum of the predictions for each augmented version of the image. 在推理或验证期间，使用数据增强创建每个图像的多个版本，然后对图像的每个增强版本取预测的平均值或最大值。\n由上面的训练可知在没有TTA的情况下错误率是：\nvalid = learn.dls.valid preds,targs = learn.get_preds(dl=valid) error_rate(preds, targs) TensorBase(0.0351)\ntta_preds,_ = learn.tta(dl=valid) error_rate(tta_preds, targs) TensorBase(0.0332)\n错误率有降低！\n将ConvNext应用的规模扩大 # trn_path = path/\u0026#39;train_images\u0026#39; learn = train(arch, epochs=12, item=Resize((480, 360), method=ResizeMethod.Pad, pad_mode=PadMode.Zeros), batch=aug_transforms(size=(256,192), min_scale=0.75)) epoch train_loss valid_loss error_rate time 0 1.097865 0.666030 0.218645 01:10 epoch train_loss valid_loss error_rate time 0 0.530017 0.320848 0.103316 01:27 1 0.399917 0.254228 0.077367 01:28 2 0.354157 0.281102 0.087938 01:27 3 0.306803 0.235106 0.074003 01:27 4 0.222612 0.154293 0.043248 01:28 5 0.158987 0.174899 0.046612 01:27 6 0.127389 0.122164 0.035079 01:28 7 0.089526 0.120237 0.030754 01:27 8 0.070281 0.117129 0.032196 01:28 9 0.052415 0.094729 0.024507 01:27 10 0.044216 0.091565 0.024027 01:28 11 0.034039 0.091716 0.023546 01:28 应用tta\ntta_preds,targs = learn.tta(dl=learn.dls.valid) error_rate(tta_preds, targs) TensorBase(0.0226)\n可以看到是降低了损失率的\n将ConvNext应用到测试集上 # tst_files = get_image_files(path/\u0026#39;test_images\u0026#39;).sorted() tst_dl = learn.dls.test_dl(tst_files) preds,_ = learn.tta(dl=tst_dl) idxs = preds.argmax(dim=1) 使用PyTorch 中的 argmax来选取出最大概率所在的标签，dim=1即沿类别维度（每行）计算，返回每个样本预测概率最高的类别索引\nvocab = np.array(learn.dls.vocab) # 使用vocab映射 results = pd.Series(vocab[idxs], name=\u0026#34;idxs\u0026#34;) 在 fastai 中，learn.dls.vocab 是数据加载器 (DataLoaders) 的类别标签列表，用于将模型输出的预测索引（数字）映射回可读的类别名称（如字符串）\nss = pd.read_csv(path/\u0026#39;sample_submission.csv\u0026#39;) ss[\u0026#39;label\u0026#39;] = results ss.to_csv(\u0026#39;subm2.csv\u0026#39;, index=False) !head subm2.csv image_id,label 200001.jpg,hispa 200002.jpg,normal 200003.jpg,blast 200004.jpg,blast 200005.jpg,blast 200006.jpg,brown_spot 200007.jpg,dead_heart 200008.jpg,brown_spot 200009.jpg,hispa\n创建多输出的模型 # 创建模型准备 # df = pd.read_csv(path/\u0026#39;train.csv\u0026#39;, index_col=\u0026#39;image_id\u0026#39;) df.head() label variety age image_id 100330.jpg bacterial_leaf_blight ADT45 45 100365.jpg bacterial_leaf_blight ADT45 45 100382.jpg bacterial_leaf_blight ADT45 45 100632.jpg bacterial_leaf_blight ADT45 45 101918.jpg bacterial_leaf_blight ADT45 45 我们让image_id作为索引，方便查找\nDataBlock使用 get_image_files 来获取训练图像列表，该列表返回 Path 对象，而如果要查找某个项目以获取其种类，我们需要传递其 name 。这是一个执行此操作的函数：\ndef get_variety(p): return df.loc[p.name, \u0026#39;variety\u0026#39;] dls = DataBlock( blocks=(ImageBlock,CategoryBlock,CategoryBlock), n_inp=1, get_items=get_image_files, get_y = [parent_label,get_variety], splitter=RandomSplitter(0.2, seed=42), item_tfms=Resize(192, method=\u0026#39;squish\u0026#39;), batch_tfms=aug_transforms(size=128, min_scale=0.75) ).dataloaders(trn_path) 使用 DataBlock API，这是一种灵活且方便的方式，可以将数据处理管道的各个部分连接在一起 DataBlock 将从每个文件中创建 3 个内容：一个图像（文件的内容）和 2 个分类变量（疾病和品种）。\ndls.show_batch(max_n=6) 定义多输出的学习模型 # 多输出的关键的区别在于，我们的指标和损失现在将接收三个东西而不是两个：模型输出（即指标和损失函数输入）和两个目标（疾病和多样性）。因此，我们需要重新定义指标（ error_rate ）和损失函数（ cross_entropy ），以传递 disease 目标：\narch = \u0026#39;convnext_small_in22k\u0026#39; lr = 0.01 为了预测每种疾病和每种疾病的概率，我们现在需要模型输出一个长度为 20 的张量，因为有 10 种可能的疾病和 10 种可能的疾病。我们可以通过设置 n_out=20\nlearn = vision_learner(dls, arch, n_out=20).to_fp16() 因为要预测两个输出了，之前的损失函数是只针对疾病的，在这里损失函数使用的是cross_entropy即交叉熵损失函数\ndef disease_loss(inp,disease,variety): return F.cross_entropy(inp[:,:10],disease) def variety_loss(inp,disease,variety): return F.cross_entropy(inp[:,10:],variety) def combine_loss(inp,disease,variety):return disease_loss(inp,disease,variety)+variety_loss(inp,disease,variety) def disease_err(inp,disease,variety): return error_rate(inp[:,:10],disease) def variety_err(inp,disease,variety): return error_rate(inp[:,10:],variety) err_metrics = (disease_err,variety_err) all_metrics = err_metrics+(disease_loss,variety_loss) 多输出模型训练 # learn = vision_learner(dls, arch, loss_func=combine_loss, metrics=all_metrics, n_out=20).to_fp16() learn.fine_tune(5, lr) epoch train_loss valid_loss disease_err variety_err disease_loss variety_loss time 0 2.276296 1.189668 0.257088 0.128304 0.783521 0.406147 00:51 epoch train_loss valid_loss disease_err variety_err disease_loss variety_loss time 0 1.005188 0.580068 0.127823 0.059106 0.387363 0.192705 00:54 1 0.757706 0.387292 0.094666 0.036040 0.280007 0.107285 00:53 2 0.477868 0.262185 0.061509 0.022585 0.193384 0.068801 00:54 3 0.269435 0.171966 0.040846 0.009611 0.134649 0.037317 00:54 4 0.200365 0.163008 0.037963 0.009130 0.124750 0.038258 00:53 添加softmax并查看输出结果 # # 获取验证集的预测 preds, targs = learn.get_preds(dl=learn.dls.valid) print(learn.dls.vocab) print(len(learn.dls.vocab)) [[\u0026lsquo;bacterial_leaf_blight\u0026rsquo;, \u0026lsquo;bacterial_leaf_streak\u0026rsquo;, \u0026lsquo;bacterial_panicle_blight\u0026rsquo;, \u0026lsquo;blast\u0026rsquo;, \u0026lsquo;brown_spot\u0026rsquo;, \u0026lsquo;dead_heart\u0026rsquo;, \u0026lsquo;downy_mildew\u0026rsquo;, \u0026lsquo;hispa\u0026rsquo;, \u0026rsquo;normal\u0026rsquo;, \u0026rsquo;tungro\u0026rsquo;], [\u0026lsquo;ADT45\u0026rsquo;, \u0026lsquo;AndraPonni\u0026rsquo;, \u0026lsquo;AtchayaPonni\u0026rsquo;, \u0026lsquo;IR20\u0026rsquo;, \u0026lsquo;KarnatakaPonni\u0026rsquo;, \u0026lsquo;Onthanel\u0026rsquo;, \u0026lsquo;Ponni\u0026rsquo;, \u0026lsquo;RR\u0026rsquo;, \u0026lsquo;Surya\u0026rsquo;, \u0026lsquo;Zonal\u0026rsquo;]] 2\n# 疾病类别有10个，品种类别也有10个 num_classes_disease = len(learn.dls.vocab[0]) # 10 num_classes_variety = len(learn.dls.vocab[1]) # 10 # 获取验证集的预测 preds, targs = learn.get_preds(dl=learn.dls.valid) # 划分疾病和品种的预测结果 disease_preds = preds[:, :num_classes_disease] # 获取第一部分，即疾病的预测 variety_preds = preds[:, num_classes_disease:] # 获取第二部分，即品种的预测 # 使用 softmax 计算概率 disease_probabilities = F.softmax(disease_preds, dim=1) variety_probabilities = F.softmax(variety_preds, dim=1) # 获取分类的最大概率对应的类别索引 predicted_disease_classes = disease_probabilities.argmax(dim=1) # 找出疾病类别的预测 predicted_variety_classes = variety_probabilities.argmax(dim=1) # 找出品种类别的预测 # 映射类别索引回实际标签 predicted_disease_labels = np.array(learn.dls.vocab[0])[predicted_disease_classes] predicted_variety_labels = np.array(learn.dls.vocab[1])[predicted_variety_classes] # 查看部分预测结果 results_df = pd.DataFrame({ \u0026#39;Predicted Disease\u0026#39;: predicted_disease_labels[:10], # 前10个的预测疾病 \u0026#39;Predicted Variety\u0026#39;: predicted_variety_labels[:10], # 前10个的预测品种 }) print(results_df) Predicted Disease Predicted Variety blast AndraPonni normal RR tungro ADT45 bacterial_leaf_streak KarnatakaPonni dead_heart ADT45 tungro ADT45 normal ADT45 normal ADT45 brown_spot ADT45 bacterial_leaf_blight ADT45 ","date":"27 March 2025","externalUrl":null,"permalink":"/posts/deeplearning/learning-based-paddydiseaseclassification/","section":"Posts","summary":"","title":"Learning NN Based Paddy_Disease_Classification","type":"posts"},{"content":" 哈希 # 本系列的大部分都是对labuladong和代码随想录网站的题目的练手汇总(部分是博主自己结合leetcode和洛谷题目的补充）初衷是博主自用学习哒!\n请所有阅读这系列博客的友友先移步阅览他们的网站，没有这些优秀网站的分享不会有这系列的文章，我只是知识的搬运工！\n两数之和 # 题目描述 # 给定一个整数数组 nums 和一个整数目标值 target，请你在该数组中找出 和为目标值 target 的那 两个 整数，并返回它们的数组下标。\n你可以假设每种输入只会对应一个答案，并且你不能使用两次相同的元素。\n你可以按任意顺序返回答案。\nAC代码(代码随想录) # class Solution { public: vector\u0026lt;int\u0026gt; twoSum(vector\u0026lt;int\u0026gt;\u0026amp; nums, int target) { unordered_map\u0026lt;int,int\u0026gt;map; for(int i=0;i\u0026lt;nums.size();i++) { int n=target-nums[i]; if(map.find(n)!=map.end())return {map.find(n)-\u0026gt;second,i}; else map.insert(pair\u0026lt;int,int\u0026gt;(nums[i],i)); } return {}; } }; P1102 A-B 数对 # 给出一串正整数数列以及一个正整数C, 要求计算出所有满足 A - B = C​ 的数对的个数（不同位置的数字一样的数对算不同的数对）。\n输入格式 # 输入共两行。\n第一行，两个正整数 N,C。\n第二行，N 个正整数，作为要求处理的那串数。\n输出格式 # 一行，表示该串正整数中包含的满足 A - B = C​ 的数对的个数。\n输入 # 4 1 1 1 2 3\n输出 # 3\n说明/提示 # 对于 75%​ 的数据，​1\u0026lt; N \u0026lt; 2000​。\n对于100%的数据，1 \u0026lt; N \u0026lt; 2×10^5​，0 \u0026lt; a_i \u0026lt; 2^30​，1 \u0026lt; C \u0026lt; 2^30​。\nAC代码 # #include \u0026lt;bits/stdc++.h\u0026gt; using namespace std; typedef long long ll; int main(){ int N, C; cin \u0026gt;\u0026gt; N \u0026gt;\u0026gt; C; vector\u0026lt;int\u0026gt; nums(N); unordered_map\u0026lt;int, int\u0026gt; freq; // 使用 int 作为频次类型 for(int i=0;i\u0026lt;N;i++){ int x; cin \u0026gt;\u0026gt; x; nums[i]=x; freq[x]++; } ll res = 0;//注意结果是long long 因为res会累加 for(const int \u0026amp;x : nums){ int target = x + C; if(freq.find(target) != freq.end()){ res += freq[target]; } } cout \u0026lt;\u0026lt; res; } ","date":"26 March 2025","externalUrl":null,"permalink":"/posts/algorithm/%E5%93%88%E5%B8%8C/","section":"Posts","summary":"","title":"哈希","type":"posts"},{"content":" 回溯 # 本系列的大部分都是对labuladong和代码随想录网站的题目的练手汇总(部分是博主自己结合leetcode和洛谷题目的补充）初衷是博主自用学习哒!\n请所有阅读这系列博客的友友先移步阅览他们的网站，没有这些优秀网站的分享不会有这系列的文章，我只是知识的搬运工！\n回溯三部曲labuladong\n1)确定回溯函数的返回值及参数\n每次回溯时要更新什么，要返回什么\n2）回溯函数的终止条件\n3）回溯搜索的遍历过程\nvector\u0026lt;vector\u0026gt; result\nvector path\nP4913 【深基16.例3】二叉树深度 # 题目描述 # 有一个 n(n\u0026lt;10^6)​ 个结点的二叉树。给出每个结点的两个子结点编号（均不超过 n），建立一棵二叉树（根节点的编号为 1​），如果是叶子结点，则输入 0 0。\n建好这棵二叉树之后，请求出它的深度。二叉树的深度是指从根节点到叶子结点时，最多经过了几层。\n输入格式 # 第一行一个整数 n​，表示结点数。\n之后 n行，第 i 行两个整数 l、r，分别表示结点 i 的左右子结点编号。若 l=0 则表示无左子结点，r=0 同理。\n输出格式 # 一个整数，表示最大结点深度。\n输入 # 7 2 7 3 6 4 5 0 0 0 0 0 0 0 0 输出 # 4 AC代码 # #include \u0026lt;iostream\u0026gt; #include \u0026lt;unordered_map\u0026gt; #include\u0026lt;math.h\u0026gt; using namespace std; unordered_map\u0026lt;int, pair\u0026lt;int, int\u0026gt;\u0026gt; tree; int maxDepth = 0; void traverse(int node, int currentDepth) { int left = tree[node].first; int right = tree[node].second; maxDepth = max(maxDepth,currentDepth); // 遍历左子树 if (left != 0) { traverse(left, currentDepth + 1); } // 遍历右子树 if (right != 0) { traverse(right, currentDepth + 1); } } int main() { int n; cin \u0026gt;\u0026gt; n; for (int i = 1; i \u0026lt;= n; ++i) { int l, r; cin \u0026gt;\u0026gt; l \u0026gt;\u0026gt; r; tree[i] = {l, r}; } traverse(1, 1); cout \u0026lt;\u0026lt; maxDepth \u0026lt;\u0026lt; endl; return 0; } P1025 [NOIP 2001 提高组] 数的划分 # 题目描述 # 将整数 n 分成 k份，且每份不能为空，任意两个方案不相同（不考虑顺序）。\n例如：n=7​，k=3，下面三种分法被认为是相同的。\n1,1,5​;\n1,5,1​;\n​5,1,1.\n问有多少种不同的分法。\n输入格式 # n,k（6\u0026lt;n\u0026lt; 200,2 \u0026lt;k \u0026lt;6）\n输出格式 # 1 个整数，即不同的分法。\n输入 # 7 3\n输出 # 4\n说明/提示 # 四种分法为：\n1,1,5;\n1,2,4;\n1,3,3;\n2,2,3.\nAC代码 # #include \u0026lt;bits/stdc++.h\u0026gt; using namespace std; int n, k, res = 0; stack\u0026lt;int\u0026gt; path; // 回溯函数 void backtrack(int start, int sum) { // 如果已经选择了k个数 if (path.size() == k) { if (sum == n) { res++; } return; } // 计算当前数最大可以是多少，避免无效递归 int remaining = k - path.size(); int max_i = (n - sum) / remaining; for(int i = start; i \u0026lt;= max_i; i++) { path.push(i); backtrack(i, sum + i); // 下一轮起始值至少为i，确保非递减!!!因为好几种算重复 path.pop(); } } int main(){ cin \u0026gt;\u0026gt; n \u0026gt;\u0026gt; k; backtrack(1, 0); cout \u0026lt;\u0026lt; res; return 0; } 未加入记忆数组，纯递归，超时：\n#include \u0026lt;iostream\u0026gt; using namespace std; int count(int num) { if (num == 0) return 0; int res = 1; // 包括自身作为一个数列的情况 for (int i = 1; i \u0026lt;= num / 2; ++i) { res += count(i); } return res; } int main() { int n; cin \u0026gt;\u0026gt; n; cout \u0026lt;\u0026lt; count(n) \u0026lt;\u0026lt; endl; return 0; } 改进：\n#include \u0026lt;iostream\u0026gt; using namespace std; int dp[100001] = {0}; // 记忆化数组 int count(int num) { if (num == 0) return 0; if (dp[num] != 0) return dp[num]; // 已计算过则直接返回 dp[num] = 1; // 初始化为1（自身） for (int i = 1; i \u0026lt;= num / 2; ++i) { dp[num] += count(i); } return dp[num]; } int main() { int n; cin \u0026gt;\u0026gt; n; cout \u0026lt;\u0026lt; count(n) \u0026lt;\u0026lt; endl; return 0; } #include \u0026lt;iostream\u0026gt; using namespace std; const int MAX_N = 100000; // 根据题目约束调整最大值 int dp[MAX_N + 1]; int main() { int n; cin \u0026gt;\u0026gt; n; dp[0] = 0; dp[1] = 1; for (int num = 2; num \u0026lt;= n; ++num) { dp[num] = 1; // 至少包含自己 for (int i = 1; i \u0026lt;= num / 2; ++i) { dp[num] += dp[i]; //求和过程 } } cout \u0026lt;\u0026lt; dp[n] \u0026lt;\u0026lt; endl; return 0; } 【蓝桥杯题库】81305. 小齐的字母方块拼写 # 问题描述 # 小齐有四个木块，每个木块是一个立方体，上面分别写有字母表的大写字母。她想通过排列这些木块，拼出一些单词。\n给定每个木块上的字母和小齐想拼出的单词列表，请确定她能成功拼出哪些单词。\n输入格式 # 第一行包含一个整数 N，表示小齐想拼出的单词数量。\n接下来的四行，每行包含一个字符串，表示一个木块上六个大写字母。\n接下来的 N行，每行包含一个小写字母数量在 1 到 4 之间的大写字母单词。\n输出格式 # 对于小齐想拼出的每个单词，如果她能成功拼出，输出 YES，否则输出 NO。\n样例输入 # 6 MOOOOO OOOOOO ABCDEF UVWXYZ COW MOO ZOO MOVE CODE FARM 样例输出 # YES NO YES YES NO NO 评测数据规模 # 1≤N≤10。\nAC代码 # #include \u0026lt;bits/stdc++.h\u0026gt; using namespace std; vector\u0026lt;string\u0026gt; table; vector\u0026lt;string\u0026gt; words; bool canSpell(const string\u0026amp; word, vector\u0026lt;bool\u0026gt;\u0026amp; used, int index) { if (index == word.size()) return true; for (int i = 0; i \u0026lt; 4; i++) { if (!used[i] \u0026amp;\u0026amp; table[i].find(word[index]) != string::npos) { //注意string也有find方法 != string::npos 和 unordered_map != map.end() 有点类似 used[i] = true; if (canSpell(word, used, index + 1)) return true; used[i] = false; } } return false; } int main() { int n; cin \u0026gt;\u0026gt; n; table.resize(4); words.resize(n); for (int i = 0; i \u0026lt; 4; i++)cin \u0026gt;\u0026gt; table[i]; for (int i = 0; i \u0026lt; n; i++) cin \u0026gt;\u0026gt; words[i]; for (int i = 0; i \u0026lt; n; i++) { vector\u0026lt;bool\u0026gt; used(4, false); if (canSpell(words[i], used, 0)) { cout \u0026lt;\u0026lt; \u0026#34;YES\u0026#34; \u0026lt;\u0026lt; endl; } else { cout \u0026lt;\u0026lt; \u0026#34;NO\u0026#34; \u0026lt;\u0026lt; endl; } } return 0; } ","date":"26 March 2025","externalUrl":null,"permalink":"/posts/algorithm/%E5%9B%9E%E6%BA%AF/","section":"Posts","summary":"","title":"回溯","type":"posts"},{"content":" 并查集 # 本系列的大部分都是对labuladong和代码随想录网站的题目的练手汇总(部分是博主自己结合leetcode和洛谷题目的补充）初衷是博主自用学习哒!\n请所有阅读这系列博客的友友先移步阅览他们的网站，没有这些优秀网站的分享不会有这系列的文章，我只是知识的搬运工！\n“并查集用于解决连通性问题”\ntemplate\njoin函数一定要先通过find函数寻根再进行关联\nint n = 1005; // n根据题目中节点数量而定，一般比节点数量大一点就好 vector\u0026lt;int\u0026gt; father (n, 0); // 并查集初始化 void init() { for (int i = 0; i \u0026lt; n; ++i) { father[i] = i; } } // 并查集里寻根的过程 int find(int u) { return u == father[u] ? u : father[u] = find(father[u]); // 路径压缩,将多层压缩为两层，只有第一层是根节点 } // 判断 u 和 v是否找到同一个根 bool isSame(int u, int v) { u = find(u); v = find(v); return u == v; } // 将v-\u0026gt;u 这条边加入并查集 void join(int u, int v) { u = find(u); // 寻找u的根 v = find(v); // 寻找v的根 if (u == v) return ; // 如果发现根相同，则说明在一个集合，不用两个节点相连直接返回 father[v] = u; } 删除冗余边Ⅱ（有向图） # 有一种有向树,该树只有一个根节点，所有其他节点都是该根节点的后继。该树除了根节点之外的每一个节点都有且只有一个父节点，而根节点没有父节点。有向树拥有 n 个节点和 n - 1 条边。输入一个有向图，该图由一个有着 n 个节点(节点编号 从 1 到 n)，n 条边，请返回一条可以删除的边，使得删除该条边之后该有向图可以被当作一颗有向树。\n输入描述\n第一行输入一个整数 N，表示有向图中节点和边的个数。\n后续 N 行，每行输入两个整数 s 和 t，代表这是 s 节点连接并指向 t 节点的单向边\n输出描述\n输出一条可以删除的边，若有多条边可以删除，请输出标准输入中最后出现的一条边。\n输入示例\n3 1 2 1 3 2 3 输出示例\n2 3\n题目分析 # 如果发现入度为2的节点，我们需要判断 删除哪一条边，删除后本图能成为有向树。如果是删哪个都可以，优先删顺序靠后的边。情况三： 如果没有入度为2的点，说明 图中有环了（注意是有向环）。\nisTreeAfterRemoveEdge() 判断删一个边之后是不是有向树： 将所有边的两端节点分别加入并查集，遇到要 要删除的边则跳过，如果遇到即将加入并查集的边的两端节点 本来就在并查集了，说明构成了环。\n如果顺利将所有边的两端节点（除了要删除的边）加入了并查集，则说明 删除该条边 还是一个有向树\ngetRemoveEdge()确定图中一定有了有向环，那么要找到需要删除的那条边： 将所有边的两端节点分别加入并查集，如果遇到即将加入并查集的边的两端节点 本来就在并查集了，说明构成了环。\nAC代码 # #include \u0026lt;iostream\u0026gt; #include \u0026lt;vector\u0026gt; using namespace std; int n; vector\u0026lt;int\u0026gt; father (1001, 0); // 并查集初始化 void init() { for (int i = 1; i \u0026lt;= n; ++i) { father[i] = i; } } // 并查集里寻根的过程 int find(int u) { return u == father[u] ? u : father[u] = find(father[u]); } // 将v-\u0026gt;u 这条边加入并查集 void join(int u, int v) { u = find(u); v = find(v); if (u == v) return ; father[v] = u; } // 判断 u 和 v是否找到同一个根 bool same(int u, int v) { u = find(u); v = find(v); return u == v; } // 在有向图里找到删除的那条边，使其变成树 void getRemoveEdge(const vector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt;\u0026amp; edges) { init(); // 初始化并查集 for (int i = 0; i \u0026lt; n; i++) { // 遍历所有的边 if (same(edges[i][0], edges[i][1])) { // 构成有向环了，就是要删除的边 cout \u0026lt;\u0026lt; edges[i][0] \u0026lt;\u0026lt; \u0026#34; \u0026#34; \u0026lt;\u0026lt; edges[i][1]; return; } else { join(edges[i][0], edges[i][1]); } } } // 删一条边之后判断是不是树 bool isTreeAfterRemoveEdge(const vector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt;\u0026amp; edges, int deleteEdge) { init(); // 初始化并查集 for (int i = 0; i \u0026lt; n; i++) { if (i == deleteEdge) continue; if (same(edges[i][0], edges[i][1])) { // 构成有向环了，一定不是树 return false; } join(edges[i][0], edges[i][1]); } return true; } int main() { int s, t; vector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt; edges; cin \u0026gt;\u0026gt; n; vector\u0026lt;int\u0026gt; inDegree(n + 1, 0); // 记录节点入度 for (int i = 0; i \u0026lt; n; i++) { cin \u0026gt;\u0026gt; s \u0026gt;\u0026gt; t; inDegree[t]++; edges.push_back({s, t}); } vector\u0026lt;int\u0026gt; vec; // 记录入度为2的边（如果有的话就两条边） // 找入度为2的节点所对应的边，注意要倒序，因为优先删除最后出现的一条边 for (int i = n - 1; i \u0026gt;= 0; i--) { if (inDegree[edges[i][1]] == 2) { vec.push_back(i); } } // 情况一、情况二 if (vec.size() \u0026gt; 0) { // 放在vec里的边已经按照倒叙放的，所以这里就优先删vec[0]这条边 if (isTreeAfterRemoveEdge(edges, vec[0])) { cout \u0026lt;\u0026lt; edges[vec[0]][0] \u0026lt;\u0026lt; \u0026#34; \u0026#34; \u0026lt;\u0026lt; edges[vec[0]][1]; } else { cout \u0026lt;\u0026lt; edges[vec[1]][0] \u0026lt;\u0026lt; \u0026#34; \u0026#34; \u0026lt;\u0026lt; edges[vec[1]][1]; } return 0; } // 处理情况三 // 明确没有入度为2的情况，那么一定有有向环，找到构成环的边返回就可以了 getRemoveEdge(edges); } 买云朵（01背包+并查集） # 题目描述 # Joe觉得云朵很美，决定去山上的商店买一些云朵。商店里有n朵云，云朵被编号为1，2，…，n，并且每朵云都有一个价值。但是商店老板跟他说，一些云朵要搭配来买才好，所以买一朵云则与这朵云有搭配的云都要买。\n但是Joe的钱有限，所以他希望买的价值越多越好。\n输入格式 # 第1行n，m，w，表示n朵云，m个搭配，Joe有w的钱。\n第 2-n+1 行，每行 ci，di 表示i朵云的价钱和价值。\n第n+2-n+1+m行，每行 ui，vi，表示买ui就必须买vi，同理，如果买vi就必须买ui。\n输出格式 # 一行，表示可以获得的最大价值。\n输入样例 # 5 3 10 3 10 3 10 3 10 5 100 10 1 1 3 3 2 4 2\n输出样例 # 1\n【提示】 【数据范围】\n30%的数据保证：n≤100；\n50%的数据保证：n≤1,000；m≤100；w≤1,000；\n100%的数据保证：n≤10,000；0≤m≤5000；w≤10,000。\nAC代码 # #include \u0026lt;bits/stdc++.h\u0026gt; using namespace std; const int MAXN = 10010; int dp[MAXN]; // DP数组，表示在钱为j时的最大价值 int father[MAXN]; // 并查集父节点 int cost[MAXN]; // 每组的成本 int value[MAXN]; // 每组的价值 vector\u0026lt;pair\u0026lt;int, int\u0026gt;\u0026gt; grid; // 存储每朵云的成本和价值 // 初始化并查集 void init(int n) { for (int i = 1; i \u0026lt;= n; i++) { father[i] = i; cost[i] = grid[i].first; value[i] = grid[i].second; } } // 查找并查集的根节点 int find(int x) { return x == father[x] ? x : father[x] = find(father[x]); } // 合并两个云朵的组 void join(int u, int v) { u = find(u); v = find(v); if (u == v) return; // 已经在同一组 father[v] = u; // 合并 cost[u] += cost[v]; // 合并成本 value[u] += value[v]; // 合并价值 } int main() { int n, m, w; cin \u0026gt;\u0026gt; n \u0026gt;\u0026gt; m \u0026gt;\u0026gt; w; grid.resize(n + 1); // 调整大小 for (int i = 1; i \u0026lt;= n; i++) { cin \u0026gt;\u0026gt; grid[i].first \u0026gt;\u0026gt; grid[i].second; } init(n); // 初始化并查集 // 处理搭配关系 for (int i = 1; i \u0026lt;= m; i++) { int u, v; cin \u0026gt;\u0026gt; u \u0026gt;\u0026gt; v; join(u, v); // 合并云朵的组 } // 动态规划求解 for (int i = 1; i \u0026lt;= n; i++) { if (father[i] == i) { // 只处理根节点 for (int j = w; j \u0026gt;= cost[i]; j--) { dp[j] = max(dp[j], dp[j - cost[i]] + value[i]); } } } // 输出最大价值 cout \u0026lt;\u0026lt; dp[w] \u0026lt;\u0026lt; endl; return 0; } 小齐的图论之旅 # 问题描述 # 小齐正在学习图论课程，并遇到了以下问题，她感到有些困惑。请你帮助她解决这个问题！\n给定一个连通的无向图，图中的顶点标号为 1…N，边标号为 1…M。对于图中的每个顶点 v，执行以下过程：\n令 S=v 且 h=0。\n当 ∣S∣\u0026lt;N 时：\n从所有与 S 中某一端点相连的边中，选择标号最小的边 e。将 e 的另一端点加入 S。\n更新h=10×h+e。\n返回 h mod 10^9+7。\n求解该过程的所有返回值。\n输入格式 # 第一行包含两个整数 N 和 M。\n接下来有M行，每行包含一条边 (a_e,b_e) 的两个端点，表示图中的一条边（1≤a_e\u0026lt;b_e≤N )。保证这些边构成一棵连通树，且每一对顶点之间最多只有一条边。\n输出格式 # 输出 N 行，其中第 i 行应包含从顶点 i 开始执行过程时的返回值。\n样例输入 # 3 2 1 2 2 3 样例输出 # 12 12 21 评测数据规模\n2≤N≤2×10^5， N−1≤M≤4×10^5。\nTLE代码 # //TLE了，仅作思路借鉴 #include \u0026lt;bits/stdc++.h\u0026gt; using namespace std; const int MOD = 1e9 + 7; struct Edge { int l, r; int val; }; struct CompareEdge { bool operator()(const Edge\u0026amp; a, const Edge\u0026amp; b) { return a.val \u0026gt; b.val; } }; int main() { int n, m; cin \u0026gt;\u0026gt; n \u0026gt;\u0026gt; m; vector\u0026lt;Edge\u0026gt; edges(m + 1, {0, 0, 0}); vector\u0026lt;vector\u0026lt;Edge\u0026gt;\u0026gt; adj(n + 1); for (int i = 1; i \u0026lt;= m; i++) { cin \u0026gt;\u0026gt; edges[i].l \u0026gt;\u0026gt; edges[i].r; edges[i].val = i; adj[edges[i].l].push_back(edges[i]); adj[edges[i].r].push_back(edges[i]); } vector\u0026lt;long long\u0026gt; result(n + 1, 0); for (int start = 1; start \u0026lt;= n; start++) { vector\u0026lt;bool\u0026gt; isinS(n + 1, false); isinS[start] = true; priority_queue\u0026lt;Edge, vector\u0026lt;Edge\u0026gt;, CompareEdge\u0026gt; pq; for (Edge\u0026amp; edge : adj[start]) { pq.push(edge); } int count = 1; long long h = 0; while (count \u0026lt; n \u0026amp;\u0026amp; !pq.empty()) { Edge edge = pq.top(); pq.pop(); int next = (isinS[edge.l]) ? edge.r : edge.l; if (isinS[next]) continue; isinS[next] = true; count++; h = (10 * h + edge.val) % MOD; for (Edge\u0026amp; edge : adj[next]) { if (!isinS[edge.l] || !isinS[edge.r]) { pq.push(edge); } } } result[start] = h; } for (int i = 1; i \u0026lt;= n; i++) { cout \u0026lt;\u0026lt; result[i] \u0026lt;\u0026lt; endl; } return 0; } ","date":"26 March 2025","externalUrl":null,"permalink":"/posts/algorithm/%E5%B9%B6%E6%9F%A5%E9%9B%86/","section":"Posts","summary":"","title":"并查集","type":"posts"},{"content":" 蓝桥杯C++题库2023-1 # 博主非ACMer，题目代码分析可能会有偏颇，建议谨慎参考博主的AC代码。分布这系列的帖子的初衷是分享官方的题库，以方便友友们备赛:)\n偶串 # 小蓝特别喜欢偶数，当他看到字符串时，他总数要检查一下是不是每种字符都是出现偶数次。给定一个字符串，请帮助小蓝检查一下该字符串是否满足要求。\n输入描述 # 输入一行包含一个字符串，由小写英文字母组成。\n输出描述 # 如果字符串中的每种字符都是出现偶数次，输出大写英文单词 YES ，否则输出大写英文单词 NO。\n样例输入 # banana 样例输出 # NO 评测用例规模 # 对于 50%的评测用例， 1≤ 字符串长度 ≤1000；\n对于所有评测用例，1≤ 字符串长度 ≤10^6 。\nAC代码 # 博主第一反应就是unordered_map，也就用它做出来了\n#include \u0026lt;bits/stdc++.h\u0026gt; using namespace std; int main() { unordered_map\u0026lt;char, int\u0026gt; map; string s; cin \u0026gt;\u0026gt; s; for (int i = 0; i \u0026lt; s.size(); i++) { map[s[i]]++; } for (auto\u0026amp; pair : map) { if (pair.second % 2 != 0) { cout \u0026lt;\u0026lt; \u0026#34;NO\u0026#34;; return 0; } } cout \u0026lt;\u0026lt; \u0026#34;YES\u0026#34;; return 0; } 划分 # 问题描述 # 给定 40个数，请将其任意划分成两组，每组至少一个元素。每组的权值为组内所有元素的和。划分的权值为两组权值的乘积。请问对于以下 40 个数，划分的权值最大为多少。\n问题分析 # 这个题先从二维数组分析，博主简单的画了个图，通过分析可以看出dp[i] [j] 是由​ dp[i-1] [j] ​和 dp[i] [j-nums [i]] 共同影响的。\n然后再压缩为一维数组解决。\n具体思路可参考代码随想录的0-1背包基础理论2,虽然是以0-1背包举例但是将二维数组压缩为一维数组的思路和注意的地方可以借鉴。\nAC代码 # 直觉就是分成两组和差不多的两部分，这样乘积最大。也就是说两部分的和尽量靠近sum/2\n采用动态规划，定义布尔数组dp[ i ] [ j ]，表示在前索引为0-i个数中，是否存在和为 j 的组合\n#include \u0026lt;iostream\u0026gt; #include \u0026lt;vector\u0026gt; #include \u0026lt;algorithm\u0026gt; using namespace std; int main() { int nums[40] = {5160, 9191, 6410, 4657, 7492, 1531, 8854, 1253, 4520, 9231, 1266, 4801, 3484, 4323, 5070, 1789, 2744, 5959, 9426, 4433, 4404, 5291, 2470, 8533, 7608, 2935, 8922, 5273, 8364, 8819, 7374, 8077, 5336, 8495, 5602, 6553, 3548, 5267, 9150, 3309}; int total_sum = 0; for (int i = 0; i \u0026lt; 40; i++) { total_sum += nums[i]; } vector\u0026lt;bool\u0026gt; dp(total_sum / 2 + 1, false); dp[0] = true; for (int i = 0; i \u0026lt; 40; i++) { for (int j = total_sum / 2; j \u0026gt;= nums[i]; j--) { if (dp[j - nums[i]]) { dp[j] = true; } } } long long max_product = 0; for (int i = total_sum / 2; i \u0026gt;= 0; i--) { if (dp[i]) { max_product = (long long)i * (total_sum - i); break; } } cout \u0026lt;\u0026lt; max_product \u0026lt;\u0026lt; endl; return 0; } 糖果分配 # 问题描述 # 两种糖果分别有9个和 16 个，要全部分给 7 个小朋友，每个小朋友得到的糖果总数最少为 2 个最多为 5 个，问有多少种不同的分法。糖果必须全部分完。\n只要有其中一个小朋友在两种方案中分到的糖果不完全相同，这两种方案就算作不同的方案。\nAC代码 # #include\u0026lt;bits/stdc++.h\u0026gt; using namespace std; int res=0; void dfs(int idx, int a_size, int b_size) { if (idx == 7) { if(a_size == 0 \u0026amp;\u0026amp; b_size == 0)res++; return; } for (int i = 0; i \u0026lt;= a_size; i++) { for (int j = 0; j \u0026lt;= b_size; j++) { if (i + j \u0026gt;= 2 \u0026amp;\u0026amp; i + j \u0026lt;= 5) { dfs(idx + 1, a_size - i, b_size - j); } } } } int main() { int a = 16, b = 9; dfs(0, a, b); cout \u0026lt;\u0026lt; res \u0026lt;\u0026lt; endl; return 0; } 混乘数字 # 问题描述 # 混乘数字的定义如下: 对于一个正整数 𝑛，如果存在正整数 𝑎,𝑏，使得𝑛=𝑎×𝑏，而且 𝑎和 𝑏的十进制数位中每个数字出现的次数之和与 𝑛中对应数字出现次数相同，则称 𝑛为混乘数字。例如，对于正整数 𝑛=126，存在 𝑎=6, 𝑏=21满足条件，因此126是一个混乘数字。又如，对于正整数 𝑛=180225，存在 𝑎=225, 𝑏=801 满足条件，因此 180225 是一个混乘数字。请你帮助计算出，1∼1000000(含)之间一共有多少个数字是混乘数字。\nAC代码 # #include \u0026lt;bits/stdc++.h\u0026gt; using namespace std; bool judge(int a, int b, int c, int* valid) { fill(valid, valid+10, 0); while(a){valid[a%10]++;a/=10;} while(b){valid[b%10]++;b/=10;} while(c){valid[c%10]--;c/=10;} for(int i=0;i\u0026lt;10;i++) if(valid[i]!=0) return false; return true; } int main() { const int n = 1e6; vector\u0026lt;bool\u0026gt; nums(n+1, false); int res = 0; for(int i=1; i\u0026lt;=n; i++){ for(int j=1; j\u0026lt;=i \u0026amp;\u0026amp; i*j\u0026lt;=n; j++){ int valid[10]={0}; if(judge(i,j,i*j,valid) \u0026amp;\u0026amp; !nums[i*j]){ nums[i*j] = true; //注意！一开始没加bool类型的nums数组，其实n是可能重复计数的 res++; } } } cout \u0026lt;\u0026lt; res; return 0; } 保险箱 # 问题描述 # 小蓝有一个保险箱，保险箱上共有 n 位数字。小蓝可以任意调整保险箱上的每个数字，每一次操作可以将其中一位增加 1 或减少 1。当某位原本为 9 或 0 时可能会向前（左边）进位/退位，当最高位（左边第一位）上的数字变化时向前的进位或退位忽略。\n示例：\n00000 的第 5 位减 1 变为 99999； 99999 的第 5 位减 1 变为 99998； 00000 的第 4 位减 1 变为 99990； 97993 的第 4 位加 1 变为 98003； 99909 的第 3 位加 1 变为 00009。 保险箱上一开始有一个数字 x，小蓝希望把它变成 y，这样才能打开它。问小蓝最少需要操作的次数。\n输入格式 # 第一行包含一个整数 n。 第二行包含一个 n 位整数 x。 第三行包含一个 n 位整数 y。 输出格式 # 输出一行包含一个整数表示答案。\n样例输入 # 5\n12349\n54321\n样例输出 # 11\n对于 30% 的评测用例，1 ≤ n ≤ 300； 对于 60% 的评测用例，1 ≤ n ≤ 3000； 对于所有评测用例，1 ≤ n ≤ 1e5，x 和 y 中仅包含数字 0 至 9，可能有前导零。 AC代码 # 这个状态方程没想出来:(\n定义动态规划状态 dp[i][j]，其中：\ni 表示当前处理到数字的第 i 位（从低位到高位，即从右到左）。 j 表示当前位的进位/退位状态： j = 0：第 i 位既没有进位也没有退位的操作数。 j = 1：第 i 位进行了进位（即当前位数字 +1 后影响了高位）的操作数。 j = 2：第 i 位进行了退位（即当前位数字 -1 后影响了高位）的操作数。 #include\u0026lt;bits/stdc++.h\u0026gt; using namespace std; int main() { int n; cin\u0026gt;\u0026gt;n; vector\u0026lt;int\u0026gt;xv(n,0),yv(n,0); vector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt;dp(n,vector\u0026lt;int\u0026gt;(3,0)); string x,y; cin\u0026gt;\u0026gt;x\u0026gt;\u0026gt;y; reverse(x.begin(),x.end()); reverse(y.begin(),y.end()); for(int i=0;i\u0026lt;n;i++) { xv[i]=x[i]-\u0026#39;0\u0026#39;; yv[i]=y[i]-\u0026#39;0\u0026#39;; } for(int i=0;i\u0026lt;n;i++) { if(i==0) { dp[i][0]=abs(xv[i]-yv[i]); dp[i][1]=yv[i]-xv[i]+10; dp[i][2]=xv[i]-yv[i]+10; } else { dp[i][0]=min({dp[i-1][0]+abs(yv[i]-xv[i]), dp[i-1][1]+abs(xv[i]+1-yv[i]), dp[i-1][2]+abs(xv[i]-1-yv[i])}); dp[i][1]=min({dp[i-1][0]+(10+yv[i]-xv[i]), dp[i-1][1]+(10+yv[i]-(xv[i]+1)), //上一位因为进位，因此xv[i]+1。又因为这一位要通过进位匹配，所以加上10 dp[i-1][2]+(10+yv[i]-(xv[i]-1))}); //以此类推 dp[i][2]=min({dp[i-1][0]+(10+xv[i]-yv[i]), dp[i-1][1]+(10+xv[i]+1-yv[i]), dp[i-1][2]+(10+xv[i]-1-yv[i])}); } } cout\u0026lt;\u0026lt;min({dp[n-1][0],dp[n-1][1],dp[n-1][2]}); return 0; } 管道 # 问题描述 # 有一根长度为 len 的横向管道，该管道按照单位长度分为 len 段，每一段的中央有一个可开关的阀门和一个检测水流的传感器。\n一开始管道是空的，位于 Li 的阀门会在 Si 时刻打开，并不断让水流入管道。\n对于位于 Li 的阀门，它流入的水在 Ti 时刻（Ti \u0026gt;= Si）会使得从第 Li - (Ti - Si) 段到第 Li + (Ti - Si) 段的传感器检测到水流。\n求管道中每一段中间的传感器都检测到有水流的最早时间。\n输入格式 # 输入的第一行包含两个整数 n, len，用一个空格分隔，分别表示会打开的阀门数和管道长度。\n接下来 n 行每行包含两个整数 Li, Si，用一个空格分隔，表示位于第 Li 段管道中央的阀门会在 Si 时刻打开。\n输出格式 # 输出一行包含一个整数表示答案。\n样例输入 # 3 10 1 1 6 5 10 2 样例输出 # 5 对于 30% 的评测用例，n \u0026lt;= 200，Si, len \u0026lt;= 3000； 对于 70% 的评测用例，n \u0026lt;= 5000，Si, len \u0026lt;= 10^5； 对于所有评测用例，1 \u0026lt;= n \u0026lt;= 10^5，1 \u0026lt;= Si, len \u0026lt;= 10^9，1 \u0026lt;= Li \u0026lt;= len，Li-1 \u0026lt; Li。 TLE代码 # （只通过了百分之60，没采用经典的区间覆盖套路）\n#include \u0026lt;bits/stdc++.h\u0026gt; using namespace std; bool judgeisfill(long long time, const vector\u0026lt;pair\u0026lt;long, long\u0026gt;\u0026gt;\u0026amp; water, long long len) { vector\u0026lt;bool\u0026gt; covered(len + 1, false); // 用于标记每一段是否被覆盖 for (int i = 0; i \u0026lt; water.size(); i++) { if (time \u0026gt;= water[i].second) { long long left = max(1LL, water[i].first - (time - water[i].second)); long long right = min(len, water[i].first + (time - water[i].second)); for (long long j = left; j \u0026lt;= right; j++) { covered[j] = true; // 标记覆盖范围 } } } for (int i = 1; i \u0026lt;= len; i++) { if (!covered[i]) return false; } return true; } int main() { int n; long long len; cin \u0026gt;\u0026gt; n \u0026gt;\u0026gt; len; vector\u0026lt;pair\u0026lt;long, long\u0026gt;\u0026gt; water(n, {0, 0}); vector\u0026lt;bool\u0026gt; pipe(n, false); for (int i = 0; i \u0026lt; n; i++) { cin \u0026gt;\u0026gt; water[i].first \u0026gt;\u0026gt; water[i].second; } long long s = 1e9; long long l = 1, r = s; while (l \u0026lt; r) { long long mid = (l + r) \u0026gt;\u0026gt; 1; if (judgeisfill(mid, water, len)) { r = mid; } else { l = mid + 1; } } cout \u0026lt;\u0026lt; l; return 0; } AC代码 # #include \u0026lt;bits/stdc++.h\u0026gt; using namespace std; bool judgeisfill(long long time, const vector\u0026lt;pair\u0026lt;long, long\u0026gt;\u0026gt;\u0026amp; water, long long len) { vector\u0026lt;pair\u0026lt;long long ,long long\u0026gt;\u0026gt; intervals; for (const auto\u0026amp; valve : water) { if (time \u0026gt;= valve.second) { // 如果当前时间大于等于阀门打开的时间 long long left = max(1LL, valve.first - (time - valve.second)); // 计算左边界 long long right = min(len, valve.first + (time - valve.second)); // 计算右边界 intervals.push_back({left, right}); } } if (intervals.empty()) return false; // 如果没有区间，直接返回 false // 按左端点排序 sort(intervals.begin(), intervals.end()); // 检查覆盖范围 long long r = intervals[0].second; // 初始最远右端点 if (intervals[0].first \u0026gt; 1) return false; // 如果第一个区间的左端点大于 1，直接返回 false for (size_t i = 1; i \u0026lt; intervals.size(); i++) { if (intervals[i].first \u0026gt; r + 1) return false; // 如果当前区间与 r 不相邻，返回 false r = max(r, intervals[i].second); // 更新最远右端点 } return r \u0026gt;= len; // 如果 r 大于等于 len，返回 true } int main() { int n; long long len; cin \u0026gt;\u0026gt; n \u0026gt;\u0026gt; len; vector\u0026lt;pair\u0026lt;long, long\u0026gt;\u0026gt; water(n); for (int i = 0; i \u0026lt; n; i++) { cin \u0026gt;\u0026gt; water[i].first \u0026gt;\u0026gt; water[i].second; } long long l = 0, r = 1e18; // 设置一个较大的上界 while (l \u0026lt; r) { long long mid = (l + r) \u0026gt;\u0026gt; 1; if (judgeisfill(mid, water, len)) { r = mid; } else { l = mid + 1; } } cout \u0026lt;\u0026lt; l \u0026lt;\u0026lt; endl; return 0; } 区间覆盖是一个经典问题。我们可以按区间的左端点来排序这些区间。我们检查这些区间是否覆盖了整个管道。如果第一个区间的左端点大于 1，那么表示管道的开始部分没有被覆盖，直接返回 false。否则我们设一个变量 r 表示可到达的最远距离，r的初始值为第一个区间的右端点。我们接着检查其他区间是否与 r相邻或重叠。如果当前区间和 r 相邻或重叠，我们将当前区间的右端点和 r取最大值。最后如果 r≥len则说明成功覆盖所有区间，否则说明没有。\n","date":"26 March 2025","externalUrl":null,"permalink":"/posts/lanqiaocup/lanqiaocup2023-1/","section":"Posts","summary":"","title":"Lanqiaocup2023-1","type":"posts"},{"content":" Titanic 简洁的神经网络复现 # 本文复现的是fastai的官方视频教程 点击此处跳转\n其官方kaggle笔记本 点击此处跳转\n本篇博客是在fastai课程基础上进行总结，先复现只有一层隐藏层的神经网络，接着在此基础上复现深度学习简易框架\nimport torch import numpy as np import pandas as pd df = pd.read_csv(\u0026#34;./train.csv\u0026#34;) df.head() 处理缺失值（使用众数） # df.isna().sum() ##OUTPUT PassengerId 0 Survived 0 Pclass 0 Name 0 Sex 0 Age 177 SibSp 0 Parch 0 Ticket 0 Fare 0 Cabin 687 Embarked 2 dtype: int64 modes = df.mode().iloc[0] #iloc[0]是指的是选择第一个众数（即第一行），并将其赋值给 modes df.fillna(modes,inplace=True) df.head() 处理数值变量（长尾效应） # df.isna().sum() ##OUTPUT PassengerId 0 Survived 0 Pclass 0 Name 0 Sex 0 Age 0 SibSp 0 Parch 0 Ticket 0 Fare 0 Cabin 0 Embarked 0 dtype: int64 df.describe() PassengerId Survived Pclass Age SibSp Parch Fare count 891.000000 891.000000 891.000000 891.000000 891.000000 891.000000 891.000000 mean 446.000000 0.383838 2.308642 28.566970 0.523008 0.381594 32.204208 std 257.353842 0.486592 0.836071 13.199572 1.102743 0.806057 49.693429 min 1.000000 0.000000 1.000000 0.420000 0.000000 0.000000 0.000000 25% 223.500000 0.000000\u003e 2.000000 22.000000 0.000000 0.000000 7.910400 50% 446.000000 0.000000 3.000000 24.000000 0.000000 0.000000 14.454200 75% 668.500000 1.000000 3.000000 35.000000 1.000000 0.000000 31.000000 max 891.000000 1.000000 3.000000 80.000000 8.000000 6.000000 512.329200 df[\u0026#34;Fare\u0026#34;]=np.log(df[\u0026#34;Fare\u0026#34;]+1) 处理文本变量 # df.describe(include=[object]) Name Sex Ticket Cabin Embarked count 891 891 891 891 891 unique 891 2 681 147 3 top Dooley, Mr. Patrick male 1601 B96 B98 S freq 1 577 7 691 646 df = pd.get_dummies(df, columns=[\u0026#34;Sex\u0026#34;, \u0026#34;Pclass\u0026#34;, \u0026#34;Embarked\u0026#34;], dtype=int) df.columns ##OUTPUT Index([\u0026#39;PassengerId\u0026#39;, \u0026#39;Survived\u0026#39;, \u0026#39;Name\u0026#39;, \u0026#39;Age\u0026#39;, \u0026#39;SibSp\u0026#39;, \u0026#39;Parch\u0026#39;, \u0026#39;Ticket\u0026#39;, \u0026#39;Fare\u0026#39;, \u0026#39;Cabin\u0026#39;, \u0026#39;Sex_female\u0026#39;, \u0026#39;Sex_male\u0026#39;, \u0026#39;Pclass_1\u0026#39;, \u0026#39;Pclass_2\u0026#39;, \u0026#39;Pclass_3\u0026#39;, \u0026#39;Embarked_C\u0026#39;, \u0026#39;Embarked_Q\u0026#39;, \u0026#39;Embarked_S\u0026#39;], dtype=\u0026#39;object\u0026#39;) df.head() added_cols =[\u0026#34;Sex_male\u0026#34;,\u0026#34;Sex_female\u0026#34;,\u0026#34;Pclass_1\u0026#34;,\u0026#34;Pclass_2\u0026#34;,\u0026#34;Pclass_3\u0026#34;,\u0026#34;Embarked_C\u0026#34;,\u0026#34;Embarked_Q\u0026#34;,\u0026#34;Embarked_S\u0026#34;] df[added_cols].head( ) Sex_male Sex_female Pclass_1 Pclass_2 Pclass_3 Embarked_C Embarked_Q Embarked_S 0 1 0 0 0 1 0 0 1 1 0 1 1 0 0 1 0 0 2 0 1 0 0 1 0 0 1 3 0 1 1 0 0 0 0 1 4 1 0 0 0 1 0 0 1 划分数据集 # from torch import tensor indep_cols=[\u0026#34;Age\u0026#34;,\u0026#34;SibSp\u0026#34;,\u0026#34;Parch\u0026#34;,\u0026#34;Fare\u0026#34;]+added_cols t_dep = tensor(df.Survived) t_indep = tensor(df[indep_cols].values, dtype=torch.float) t_dep.shape torch.Size([891])\nt_indep.shape torch.Size([891, 12])\nt_indep ##OUTPUT tensor([[22., 1., 0., ..., 0., 0., 1.], [38., 1., 0., ..., 1., 0., 0.], [26., 0., 0., ..., 0., 0., 1.], ..., [24., 1., 2., ..., 0., 0., 1.], [26., 0., 0., ..., 1., 0., 0.], [32., 0., 0., ..., 0., 1., 0.]]) from fastai.data.transforms import RandomSplitter trn_split,val_split = RandomSplitter(seed=42)(df) trn_indep = t_indep[trn_split] val_indep = t_indep[val_split] trn_dep = t_dep[trn_split] val_dep = t_dep[val_split] len(trn_dep) 713\nlen(val_dep) 178\ntrn_dep = trn_dep[:,None] #升维，转成矩阵 vla_dep = trn_dep[:,None] 定义训练函数和验证函数 # 训练模型 # def train_model(epochs=30,lr=1.4): torch.manual_seed(442) #能复现训练过程 coeffs = init_coeffs() for i in range(epochs): one_epoch(coeffs,lr=lr) return coeffs def init_coeffs(n_hidden=20): layer1=(torch.rand(n_coeffs,n_hidden)-0.5)/n_hidden layer2=torch.rand(n_hidden,1)-0.3 const=torch.rand(1)[0] #[0]的作用是在张量中取标量 return layer1.requires_grad_(),layer2.requires_grad_(),const.requires_grad_() def one_epoch(coeffs,lr): loss = calc_loss(coeffs,trn_indep,trn_dep) loss.backward() with torch.no_grad():update_coeffs(coeffs,lr) print(f\u0026#34;{loss:.3f}\u0026#34;,end=\u0026#34;; \u0026#34;) def update_coeffs(coeffs,lr): for layer in coeffs: layer.sub_(layer.grad*lr) layer.grad.zero_() def calc_loss(coeffs,indeps,deps): return torch.abs(calc_preds(coeffs,indeps)-deps).mean() import torch.nn.functional as F def calc_preds(coeffs, indeps): l1,l2,const = coeffs res = F.relu(indeps@l1) res = res@l2 + const return torch.sigmoid(res) 验证模型 # def acc(coeffs): return(val_dep.bool()==(calc_preds(coeffs,val_indep)\u0026gt;0.5)).float().mean() 开始训练 # n_coeffs = trn_indep.shape[1] coeffs = train_model(lr=2.4) 0.579; 0.370; 0.369; 0.369; 0.368; 0.368; 0.367; 0.366; 0.366; 0.365; 0.365; 0.364; 0.363; 0.363; 0.362; 0.361; 0.361; 0.360; 0.359; 0.358; 0.357; 0.357; 0.356; 0.355; 0.354; 0.352; 0.351; 0.349; 0.347; 0.344;\nacc(coeffs) tensor(0.5322)\ncoeffs = train_model(lr=1.2) 0.579; 0.377; 0.370; 0.369; 0.369; 0.369; 0.368; 0.368; 0.368; 0.367; 0.367; 0.366; 0.366; 0.366; 0.365; 0.365; 0.365; 0.364; 0.364; 0.364; 0.363; 0.363; 0.363; 0.362; 0.362; 0.361; 0.361; 0.361; 0.360; 0.360;\nacc(coeffs) tensor(0.5869)\n深度学习 # def init_coeffs(): hiddens = [10, 10] sizes = [n_coeffs] + hiddens + [1] n = len(sizes) layers = [(torch.rand(sizes[i], sizes[i+1])-0.3)/sizes[i+1]*4 for i in range(n-1)] consts = [(torch.rand(1)[0]-0.5)*0.1 for i in range(n-1)] for l in layers+consts: l.requires_grad_() return layers,consts import torch.nn.functional as F def calc_preds(coeffs, indeps): layers,consts = coeffs n = len(layers) res = indeps for i,l in enumerate(layers): res = res@l + consts[i] if i!=n-1: res = F.relu(res) return torch.sigmoid(res) def update_coeffs(coeffs, lr): layers,consts = coeffs for layer in layers+consts: layer.sub_(layer.grad * lr) layer.grad.zero_() n_coeffs = trn_indep.shape[1] coeffs = train_model(lr=4) 0.373; 0.377; 0.377; 0.375; 0.371; 0.368; 0.365; 0.361; 0.357; 0.357; 0.356; 0.350; 0.344; 0.619; 0.340; 0.621; 0.621; 0.620; 0.618; 0.613; 0.569; 0.379; 0.379; 0.379; 0.379; 0.379; 0.379; 0.379; 0.379; 0.379;\nacc(coeffs) tensor(0.5955)\n","date":"24 March 2025","externalUrl":null,"permalink":"/posts/deeplearning/create-neural-net-from-scratch/","section":"Posts","summary":"","title":"Create Neural Net From Scratch","type":"posts"},{"content":"","externalUrl":null,"permalink":"/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":"","externalUrl":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"}]