
[{"content":" Titanic 简洁的神经网络复现 # 本文复现的是fastai的官方视频教程 点击此处跳转\n其官方kaggle笔记本 点击此处跳转\n本篇博客是在fastai课程基础上进行总结，先复现只有一层隐藏层的神经网络，接着在此基础上复现深度学习简易框架\nimport torch import numpy as np import pandas as pd df = pd.read_csv(\u0026#34;./train.csv\u0026#34;) df.head() 处理缺失值（使用众数） # df.isna().sum() ##OUTPUT PassengerId 0 Survived 0 Pclass 0 Name 0 Sex 0 Age 177 SibSp 0 Parch 0 Ticket 0 Fare 0 Cabin 687 Embarked 2 dtype: int64 modes = df.mode().iloc[0] #iloc[0]是指的是选择第一个众数（即第一行），并将其赋值给 modes df.fillna(modes,inplace=True) df.head() 处理数值变量（长尾效应） # df.isna().sum() ##OUTPUT PassengerId 0 Survived 0 Pclass 0 Name 0 Sex 0 Age 0 SibSp 0 Parch 0 Ticket 0 Fare 0 Cabin 0 Embarked 0 dtype: int64 df.describe() PassengerId Survived Pclass Age SibSp Parch Fare count 891.000000 891.000000 891.000000 891.000000 891.000000 891.000000 891.000000 mean 446.000000 0.383838 2.308642 28.566970 0.523008 0.381594 32.204208 std 257.353842 0.486592 0.836071 13.199572 1.102743 0.806057 49.693429 min 1.000000 0.000000 1.000000 0.420000 0.000000 0.000000 0.000000 25% 223.500000 0.000000\u003e 2.000000 22.000000 0.000000 0.000000 7.910400 50% 446.000000 0.000000 3.000000 24.000000 0.000000 0.000000 14.454200 75% 668.500000 1.000000 3.000000 35.000000 1.000000 0.000000 31.000000 max 891.000000 1.000000 3.000000 80.000000 8.000000 6.000000 512.329200 df[\u0026#34;Fare\u0026#34;]=np.log(df[\u0026#34;Fare\u0026#34;]+1) 处理文本变量 # df.describe(include=[object]) Name Sex Ticket Cabin Embarked count 891 891 891 891 891 unique 891 2 681 147 3 top Dooley, Mr. Patrick male 1601 B96 B98 S freq 1 577 7 691 646 df = pd.get_dummies(df, columns=[\u0026#34;Sex\u0026#34;, \u0026#34;Pclass\u0026#34;, \u0026#34;Embarked\u0026#34;], dtype=int) df.columns ##OUTPUT Index([\u0026#39;PassengerId\u0026#39;, \u0026#39;Survived\u0026#39;, \u0026#39;Name\u0026#39;, \u0026#39;Age\u0026#39;, \u0026#39;SibSp\u0026#39;, \u0026#39;Parch\u0026#39;, \u0026#39;Ticket\u0026#39;, \u0026#39;Fare\u0026#39;, \u0026#39;Cabin\u0026#39;, \u0026#39;Sex_female\u0026#39;, \u0026#39;Sex_male\u0026#39;, \u0026#39;Pclass_1\u0026#39;, \u0026#39;Pclass_2\u0026#39;, \u0026#39;Pclass_3\u0026#39;, \u0026#39;Embarked_C\u0026#39;, \u0026#39;Embarked_Q\u0026#39;, \u0026#39;Embarked_S\u0026#39;], dtype=\u0026#39;object\u0026#39;) df.head() added_cols =[\u0026#34;Sex_male\u0026#34;,\u0026#34;Sex_female\u0026#34;,\u0026#34;Pclass_1\u0026#34;,\u0026#34;Pclass_2\u0026#34;,\u0026#34;Pclass_3\u0026#34;,\u0026#34;Embarked_C\u0026#34;,\u0026#34;Embarked_Q\u0026#34;,\u0026#34;Embarked_S\u0026#34;] df[added_cols].head( ) Sex_male Sex_female Pclass_1 Pclass_2 Pclass_3 Embarked_C Embarked_Q Embarked_S 0 1 0 0 0 1 0 0 1 1 0 1 1 0 0 1 0 0 2 0 1 0 0 1 0 0 1 3 0 1 1 0 0 0 0 1 4 1 0 0 0 1 0 0 1 划分数据集 # from torch import tensor indep_cols=[\u0026#34;Age\u0026#34;,\u0026#34;SibSp\u0026#34;,\u0026#34;Parch\u0026#34;,\u0026#34;Fare\u0026#34;]+added_cols t_dep = tensor(df.Survived) t_indep = tensor(df[indep_cols].values, dtype=torch.float) t_dep.shape torch.Size([891])\nt_indep.shape torch.Size([891, 12])\nt_indep ##OUTPUT tensor([[22., 1., 0., ..., 0., 0., 1.], [38., 1., 0., ..., 1., 0., 0.], [26., 0., 0., ..., 0., 0., 1.], ..., [24., 1., 2., ..., 0., 0., 1.], [26., 0., 0., ..., 1., 0., 0.], [32., 0., 0., ..., 0., 1., 0.]]) from fastai.data.transforms import RandomSplitter trn_split,val_split = RandomSplitter(seed=42)(df) trn_indep = t_indep[trn_split] val_indep = t_indep[val_split] trn_dep = t_dep[trn_split] val_dep = t_dep[val_split] len(trn_dep) 713\nlen(val_dep) 178\ntrn_dep = trn_dep[:,None] #升维，转成矩阵 vla_dep = trn_dep[:,None] 定义训练函数和验证函数 # 训练模型 # def train_model(epochs=30,lr=1.4): torch.manual_seed(442) #能复现训练过程 coeffs = init_coeffs() for i in range(epochs): one_epoch(coeffs,lr=lr) return coeffs def init_coeffs(n_hidden=20): layer1=(torch.rand(n_coeffs,n_hidden)-0.5)/n_hidden layer2=torch.rand(n_hidden,1)-0.3 const=torch.rand(1)[0] #[0]的作用是在张量中取标量 return layer1.requires_grad_(),layer2.requires_grad_(),const.requires_grad_() def one_epoch(coeffs,lr): loss = calc_loss(coeffs,trn_indep,trn_dep) loss.backward() with torch.no_grad():update_coeffs(coeffs,lr) print(f\u0026#34;{loss:.3f}\u0026#34;,end=\u0026#34;; \u0026#34;) def update_coeffs(coeffs,lr): for layer in coeffs: layer.sub_(layer.grad*lr) layer.grad.zero_() def calc_loss(coeffs,indeps,deps): return torch.abs(calc_preds(coeffs,indeps)-deps).mean() import torch.nn.functional as F def calc_preds(coeffs, indeps): l1,l2,const = coeffs res = F.relu(indeps@l1) res = res@l2 + const return torch.sigmoid(res) 验证模型 # def acc(coeffs): return(val_dep.bool()==(calc_preds(coeffs,val_indep)\u0026gt;0.5)).float().mean() 开始训练 # n_coeffs = trn_indep.shape[1] coeffs = train_model(lr=2.4) 0.579; 0.370; 0.369; 0.369; 0.368; 0.368; 0.367; 0.366; 0.366; 0.365; 0.365; 0.364; 0.363; 0.363; 0.362; 0.361; 0.361; 0.360; 0.359; 0.358; 0.357; 0.357; 0.356; 0.355; 0.354; 0.352; 0.351; 0.349; 0.347; 0.344;\nacc(coeffs) tensor(0.5322)\ncoeffs = train_model(lr=1.2) 0.579; 0.377; 0.370; 0.369; 0.369; 0.369; 0.368; 0.368; 0.368; 0.367; 0.367; 0.366; 0.366; 0.366; 0.365; 0.365; 0.365; 0.364; 0.364; 0.364; 0.363; 0.363; 0.363; 0.362; 0.362; 0.361; 0.361; 0.361; 0.360; 0.360;\nacc(coeffs) tensor(0.5869)\n深度学习 # def init_coeffs(): hiddens = [10, 10] sizes = [n_coeffs] + hiddens + [1] n = len(sizes) layers = [(torch.rand(sizes[i], sizes[i+1])-0.3)/sizes[i+1]*4 for i in range(n-1)] consts = [(torch.rand(1)[0]-0.5)*0.1 for i in range(n-1)] for l in layers+consts: l.requires_grad_() return layers,consts import torch.nn.functional as F def calc_preds(coeffs, indeps): layers,consts = coeffs n = len(layers) res = indeps for i,l in enumerate(layers): res = res@l + consts[i] if i!=n-1: res = F.relu(res) return torch.sigmoid(res) def update_coeffs(coeffs, lr): layers,consts = coeffs for layer in layers+consts: layer.sub_(layer.grad * lr) layer.grad.zero_() n_coeffs = trn_indep.shape[1] coeffs = train_model(lr=4) 0.373; 0.377; 0.377; 0.375; 0.371; 0.368; 0.365; 0.361; 0.357; 0.357; 0.356; 0.350; 0.344; 0.619; 0.340; 0.621; 0.621; 0.620; 0.618; 0.613; 0.569; 0.379; 0.379; 0.379; 0.379; 0.379; 0.379; 0.379; 0.379; 0.379;\nacc(coeffs) tensor(0.5955)\n","date":"24 March 2025","externalUrl":null,"permalink":"/posts/create-neural-net-from-scratch/","section":"Posts","summary":"","title":"Create Neural Net From Scratch","type":"posts"},{"content":"","date":"24 March 2025","externalUrl":null,"permalink":"/posts/","section":"Posts","summary":"","title":"Posts","type":"posts"},{"content":"","date":"24 March 2025","externalUrl":null,"permalink":"/","section":"春日汀","summary":"","title":"春日汀","type":"page"},{"content":"","externalUrl":null,"permalink":"/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":"","externalUrl":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":"","externalUrl":null,"permalink":"/series/","section":"Series","summary":"","title":"Series","type":"series"},{"content":"","externalUrl":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"}]